{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb26836b-42b2-42f4-85c0-787301e1bb81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 作業工程計画\n",
    "2022.12.27作成<br>\n",
    "1. 採点者品質予測_22冬納品_整形済み学習データを使用。21年と22夏データ。に22冬データをくっつけた\n",
    "2. 設問情報を含む科目別データフレームと、設問情報を含まない科目混合データフレームの4種類を作成\n",
    "\n",
    "reference:https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df25fb-105d-4402-aeb6-60423aca5698",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1cd9daa-82f5-4a6f-904d-3ea16baff717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "import re \n",
    "import scipy as sp\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "機械学習ライブラリの準備\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report #2値分類評価指標を出力\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor #決定木\n",
    "import lightgbm as lgb #lightGBM\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2d8db-1918-4d58-ad4f-7155bb9d7419",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 村上さん pandas_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096a0122-aefd-40e6-8647-511c744debd9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "村上さんtoolbox\n",
    "\"\"\"\n",
    "# pandas 基礎集計クラス\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "#http://qiita.com/tanemaki/items/2ed05e258ef4c9e6caac\n",
    "\n",
    "# Jupyterで表示するためには、最初に以下を実行すること\n",
    "%matplotlib inline\n",
    "\n",
    "# Static Classで設計する\n",
    "class pandas_tool:\n",
    "    \n",
    "    # All in one チェック（Jupyterのみ）\n",
    "    def all_basic_summary_jupyter(df):\n",
    "        print(\"■ 型の確認\")\n",
    "        display(pandas_tool.type(df))\n",
    "        print(\"■ 数値型の確認\")\n",
    "        display(pandas_tool.summary(df)[0])\n",
    "        print(\"■ カテゴリ型の確認\")\n",
    "        cate_var_data = list(df.select_dtypes(include=['object']).columns)\n",
    "        ret = pandas_tool.freq(df,cate_var_data)\n",
    "        for d in ret:\n",
    "            display(pd.DataFrame(d))\n",
    "            print(\"---------------\")\n",
    "        print(\"■ 欠損の確認\")\n",
    "        display(pandas_tool.check_missing(df))\n",
    "    \n",
    "    # 相関関係可視化（Jupyterのみ）\n",
    "    def all_value_relation_visualize(df):\n",
    "        #sns.set_context(\"poster\", 1.2, {\"lines.linewidth\": 3})\n",
    "        sns.pairplot(df,size=5)\n",
    "    \n",
    "    # カテゴリ変数でのヒートマップ（Jupyterのみ）\n",
    "    def make_heatmap(df,x,y,value):\n",
    "        target_df = df.pivot_table(index=x,values=value,columns=y)\n",
    "        sns.heatmap(target_df, annot=True, fmt='1.1f', cmap='Blues')\n",
    "    \n",
    "    # 散布図（Jupyterのみ）\n",
    "    def make_scatter_chart(df,x,y):\n",
    "        #sns.jointplot(x=x, y=y, data=df, kind=\"hex\")\n",
    "        sns.jointplot(x=x, y=y, data=df)\n",
    "    \n",
    "    # 組み合わせでヒートマップを作成（Jupyterのみ）\n",
    "    def all_make_heatmap(df,var_list,value):\n",
    "        col_num = 2\n",
    "        var_list_set = list(combinations(var_list,2))\n",
    "        \n",
    "        fig, axes = plt.subplots(int(len(var_list_set)/col_num)+1, col_num, figsize=(18,3+6.5*int(len(var_list_set)/col_num)))\n",
    "        \n",
    "        for i,target in enumerate(var_list_set):\n",
    "            target_df = df.pivot_table(index=target[0],values=value,columns=target[1])\n",
    "            sns.heatmap(target_df, annot=True, fmt='1.1f', cmap='Blues', ax=axes[int(i/col_num), i%col_num])\n",
    "            \n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # 数値集計\n",
    "    def summary(df,view=False):\n",
    "        ret=df.describe()\n",
    "        mis_ret=df.isnull().sum()\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(\"・統計量\")\n",
    "            print(ret)\n",
    "            print(\"・欠損値\")\n",
    "            print(mis_ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        return ret,mis_ret\n",
    "    \n",
    "    # 型チェック\n",
    "    def type(df,view=False):\n",
    "        ret = df.dtypes\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_rows\")\n",
    "            pd.set_option(\"display.max_rows\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_rows\",param)\n",
    "        return ret\n",
    "    \n",
    "    # 欠損チェック\n",
    "    def check_missing(df,view=False):\n",
    "        not_null_df=df.notnull()\n",
    "        ret=pd.DataFrame()\n",
    "        for name in not_null_df.columns:\n",
    "            tmp_df=not_null_df[name].value_counts()\n",
    "            tmp_df.name=name\n",
    "            ret = pd.concat([ret,tmp_df],axis=1)\n",
    "        \n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # 欠損値のオブザベーションを抽出\n",
    "    def get_miss_data(df,column,view=False):\n",
    "        ret=df[df[column].isnull()]\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        return ret\n",
    "    \n",
    "    # 欠損値を中央値で補完\n",
    "    def fill_miss_med(df,var_name):\n",
    "        var=df[var_name].median()\n",
    "        df[var_name].fillna(var,inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # 欠損値を0で補完\n",
    "    def fill_miss_zero(df,var_name):\n",
    "        df[var_name].fillna(0,inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # 特定の値を欠損とみなす\n",
    "    def apply_miss_value(df,var_name,value):\n",
    "        df[var_name]=df[var_name].replace(value,np.nan)\n",
    "        return df\n",
    "    \n",
    "    # 重複チェック\n",
    "    def check_dup(df,columns,view=False):\n",
    "        ret=pd.DataFrame()\n",
    "        for name in columns:\n",
    "            dup_cnt=df[name].duplicated().sum()\n",
    "            tmp_df = pd.DataFrame({'var_name':[name],'dup_cnt':[dup_cnt]})\n",
    "            ret = pd.concat([ret,tmp_df],axis=0,ignore_index= True)\n",
    "        \n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # 組み合わせ重複チェック\n",
    "    def check_dup_comb(df,columns,view=False):\n",
    "        ret = df[columns].duplicated().sum()\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # ユニークデータ取得\n",
    "    def get_uniq_data(df,uniq_key,sort_key,keep='first'):\n",
    "        ret = df.sort_values(by=sort_key)\n",
    "        ret.drop_duplicates(subset=uniq_key, keep=keep, inplace=True)\n",
    "        return ret\n",
    "    \n",
    "    # カテゴリ集計\n",
    "    def freq(df,columns,view=False):\n",
    "        ret=list()\n",
    "        for name in columns:\n",
    "            tmp_df=df[name].value_counts()\n",
    "            tmp_df.name=name\n",
    "            #ret = pd.concat([ret,tmp_df],axis=1)\n",
    "            ret.append(tmp_df)\n",
    "        \n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            for r in ret:\n",
    "                print(r)\n",
    "                #display(r)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # 複雑な集計\n",
    "    def tabulate(df,row,col=None,var='',func=np.sum,view=False):\n",
    "        if var == '':\n",
    "            tmp_df=df.reset_index(drop=False,inplace=False)\n",
    "            ret=pd.pivot_table(data=tmp_df, values='index', index=row, columns=col, aggfunc='count', dropna=False, fill_value=0 ,margins = False)\n",
    "            tmp_df=None\n",
    "        else:\n",
    "            ret=pd.pivot_table(data=df, values=var, index=row, columns=col, aggfunc=func, dropna=False, fill_value=0 ,margins = False)\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # マージ\n",
    "    def merge(df1,df2,key,how,view=True):\n",
    "        if view:\n",
    "            print(\"df1のキー重複\")\n",
    "            pandas_tool.check_dup_comb(df1,key,True)\n",
    "            print(\"df2のキー重複\")\n",
    "            pandas_tool.check_dup_comb(df2,key,True)\n",
    "            \n",
    "            print(\"df1のオブザベーション:{0}\".format(len(df1)))\n",
    "            print(\"df2のオブザベーション:{0}\".format(len(df2)))\n",
    "        \n",
    "        ret=pd.merge(df1,df2,how=how,on=key)\n",
    "        \n",
    "        if view:\n",
    "            print(\"mergeのオブザベーション:{0}\".format(len(ret)))\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # Rank\n",
    "    def rank(df,var,num,suffix='_rank',check=False):\n",
    "        labels=[i for i in range(0,num)]\n",
    "        df[var+suffix]=pd.qcut(df[var], num, labels=labels)\n",
    "        \n",
    "        # check data\n",
    "        if check:\n",
    "            ret=pd.DataFrame()\n",
    "            max_df=pandas_tool.tabulate(df=df,row=[var+suffix],var=var,func=np.max,view=False)\n",
    "            max_df.name='max'\n",
    "            min_df=pandas_tool.tabulate(df=df,row=[var+suffix],var=var,func=np.min,view=False)\n",
    "            min_df.name='min'\n",
    "            cnt_df=pandas_tool.tabulate(df=df,row=[var+suffix],var=var,func='count',view=False)\n",
    "            cnt_df.name='count'\n",
    "            ret=pd.concat([ret,min_df,max_df,cnt_df],axis=1)\n",
    "            return df,ret\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    # Rank適用(min基準)\n",
    "    def apply_rank(df,rank_df):\n",
    "        tmp_df=copy.deepcopy(rank_df)\n",
    "        tmp_df.reset_index(drop=False,inplace=True)\n",
    "        target_name=tmp_df.columns[3]\n",
    "        tmp_df.columns=[\"rank\",\"min\",\"max\",\"cnt\"]\n",
    "        \n",
    "        def judge_thld(row):\n",
    "            ret_var = -1\n",
    "            cond_list = [\"if 0 : ret_var = 0\"]\n",
    "            \n",
    "            for i in range(1,len(tmp_df)):\n",
    "                cond_list.append(\"elif row < \" +str(tmp_df.ix[i,'min'])+ \" : ret_var = \" + str(tmp_df.ix[i-1,'rank']))\n",
    "            \n",
    "            cond_list.append(\"else: ret_var = \" + str(tmp_df.ix[len(tmp_df)-1,'rank']))\n",
    "            cond_str=\"\\r\\n\".join(cond_list)\n",
    "            # ローカル辞書をexecと共有する\n",
    "            local_dict=locals()\n",
    "            exec(cond_str,local_dict)\n",
    "            return local_dict[\"ret_var\"]\n",
    "        \n",
    "        df[target_name+\"_rank\"]=df[target_name].apply(judge_thld)\n",
    "        return df\n",
    "    \n",
    "    # Min%以下はMin%点に、Max%以上はMax%点にクリップする\n",
    "    def clip_min_max(df,col_list,apply_df=None,max_pct=0.99,min_pct=0.01):\n",
    "        p_min = df[col_list].quantile(min_pct)\n",
    "        p_max = df[col_list].quantile(max_pct)\n",
    "        \n",
    "        df[col] = df[col_list].clip(p_min,p_max,axis=1)\n",
    "        \n",
    "        # もしも適用先のデータがあるならば（例えば検証データ）対応\n",
    "        if apply_df is not None:\n",
    "            apply_df[col] = apply_df[col_list].clip(p_min,p_max,axis=1)\n",
    "            return df,apply_df\n",
    "        else:\n",
    "            return df\n",
    "    \n",
    "    \n",
    "    # 文字列→数値変換\n",
    "    def conv_float(df,column,percent_flg=False):\n",
    "        \n",
    "        def conv_f(row):\n",
    "            if row[column] == \"\" or row[column] is np.nan:\n",
    "                return np.nan\n",
    "            else:\n",
    "                return float(row[column])\n",
    "        \n",
    "        df[column]=df[column].str.replace(\"\\\\\",\"\").str.replace(\",\",\"\").str.replace(\"%\",\"\").str.strip()\n",
    "        df[column]=df.apply(conv_f,axis=1)\n",
    "        \n",
    "        if percent_flg:\n",
    "            df[column]=df[column]/100\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1311f4ba-0355-4dfc-b844-f4027fc3cbdc",
   "metadata": {},
   "source": [
    "# エクセルデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36e3690-a02b-4ee9-8d0a-b776af769012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#整形済みデータの読み込み\n",
    "\n",
    "\"\"\"\n",
    "1. 秋22受領データを読み込む\n",
    "2.　必要な列名を統一する\n",
    "前処理データと受領データをくっつけるか、くっつけてからまとめて前処理するかはデータ受領後に考える。\n",
    "\"\"\"\n",
    "# \n",
    "path_name = \"/Users/s.ogura/Documents/CRLEA/data/intermediatedata/採点者品質予測_22冬納品_整形済み学習データ_参加回数修正済み_v1.xlsx\"\n",
    "path_file =  r'{}'.format(path_name)\n",
    "input_file = pd.ExcelFile(path_file)\n",
    "sn_list = input_file.sheet_names\n",
    "\n",
    "#夏採点を読み込んで、採点回を列に追加する　df_summer\n",
    "\n",
    "df = []\n",
    "df = pd.read_excel(path_file, sn_list[0], header=0, index_col=None,skiprows=None,\n",
    "                   usecols=['スタッフコード', '年度', '採点回', '学年', '科目', '分野', '身分', '年齢',\n",
    "                            '偏差値', '登録試験点数', '参加回数', '完全一致率', 'Error', 'data'])#\n",
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41da94be-12d4-4e66-a6e1-044f8461352a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['スタッフコード', '年度', '採点回', '学年', '科目', '分野', '身分', '年齢', '偏差値',\n",
       "       '登録試験点数', '参加回数', '完全一致率', 'Error', 'data'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd00f9b-d8a2-41b0-8b7e-61796e4e09e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "スタッフコード      int64\n",
       "年度           int64\n",
       "採点回         object\n",
       "学年         float64\n",
       "科目          object\n",
       "分野          object\n",
       "身分          object\n",
       "年齢         float64\n",
       "偏差値        float64\n",
       "登録試験点数       int64\n",
       "参加回数         int64\n",
       "完全一致率      float64\n",
       "Error        int64\n",
       "data         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeaaa131-12e1-4465-a7c4-7d49b681b9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24850"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498fa47-77fb-43f6-9da5-5d03ee81a65c",
   "metadata": {},
   "source": [
    "# データ前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a732ab4-887c-4622-aa18-fa4387807755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_error(df_input):\n",
    "    df = df_input.copy()\n",
    "    df = df[df['Error']==0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a336e08-7df7-4849-b993-49bdd0d350f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pret_data_keiken(df_input):\n",
    "    \"\"\"\n",
    "    同一採点者を取得\n",
    "    完全一致率の欠けているレコードを削除\n",
    "    一つ前の参加回数に、今回の参加回数を結合\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    temp=[]\n",
    "    s = list(sorted(df['スタッフコード'].unique().astype(int)))#スタッフコード単位で繰り返す\n",
    "    for h in s:\n",
    "        df1 = df[df['スタッフコード']==h].loc[:,['スタッフコード','科目','参加回数','分野','年度','採点回','学年',\n",
    "                                          '登録試験点数','身分', '年齢', '偏差値','完全一致率','data']]\n",
    "        df1 = df1[df1['完全一致率']!= 0]#完全一致率が0でないデータ\n",
    "        df1 = df1.sort_values('参加回数', ascending=True)#参加回数で並びかえる\n",
    "        df1['No']= range(1, len(df1) + 1)#同一スタッフコードの人の、完全一致率をもつレコードに連番をふる\n",
    "    \n",
    "        n = list(sorted(df1['No'].unique().astype(int)))\n",
    "    \n",
    "        for i in n:\n",
    "            df2 = df1.loc[:,['スタッフコード','参加回数','分野','完全一致率','No']]\n",
    "            df2 = df2[df2['No']== i]\n",
    "            df2 = df2.rename(columns = {'参加回数':'前回の参加回数',\n",
    "                                    '分野':'前回の分野',\n",
    "                                    '完全一致率':'前回の完全一致率'})\n",
    "            df2['前回の参加回数'] = df2['前回の参加回数'].astype(int)\n",
    "            df2 = df2.drop(columns = 'No')\n",
    "        \n",
    "            df3 = df1.loc[:,['スタッフコード','科目','参加回数','分野','年度','採点回','学年','完全一致率',\n",
    "                        '登録試験点数','身分', '年齢', '偏差値','data','No']]\n",
    "            df3 = df3[df3['No']== (i+1)]\n",
    "            df3 = df3.rename(columns = {'参加回数':'今回の参加回数',\n",
    "                                    '分野':'今回の分野',                                    \n",
    "                                    '採点回':'今回の採点回',\n",
    "                                    '年度':'今回の年度',\n",
    "                                    '完全一致率':'今回の完全一致率'})\n",
    "            df3['今回の参加回数'] = df3['今回の参加回数'].astype(int)\n",
    "            df3 = df3.drop(columns = 'No')\n",
    "        \n",
    "            ret = pd.merge(df2,df3,on = 'スタッフコード',how = 'inner')\n",
    "            temp.append(ret)\n",
    "    dfs = temp[0] #集計結果を一つのDFに結合\n",
    "    for i in temp[1:]:\n",
    "        dfs = dfs.append(i)\n",
    "    dfs = dfs.dropna(subset=['前回の完全一致率'])\n",
    "    #df = []\n",
    "    #df = dfs.drop_duplicates()\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f93ad4b6-daac-4e69-b608-78aa4c0918a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pret_data_shinjin(df_input):#修正\n",
    "    \"\"\"\n",
    "    新人のデータだけ抽出\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    df = df.loc[:,['スタッフコード','参加回数','科目','分野','年度','採点回','学年','完全一致率','登録試験点数','身分', '年齢', '偏差値','data']]\n",
    "    df = df[df['参加回数']==0] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1588c883-51f1-453a-8577-561f081d5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pret_data_etc(df_input):#経験者だが、過去データに前回のデータがない採点者\n",
    "    \"\"\"\n",
    "    経験者だが、過去データに前回のデータがない採点者を抽出\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    df = df.loc[:,['スタッフコード','参加回数','科目','分野','年度','採点回','学年','完全一致率','登録試験点数','身分', '年齢', '偏差値','data']]\n",
    "    #スタッフコードの重複があるデータを全て削除\n",
    "    df = df.drop_duplicates(subset='スタッフコード',keep=False)\n",
    "    #新人データを削除\n",
    "    df = df[~(df['参加回数']==0)] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dac91c-d5f9-4f63-8fc3-edcb03d4282b",
   "metadata": {},
   "source": [
    "# 前処理済みデータの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0931c670-d3b1-4f64-b8de-edafd8bc6e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.88842797279358\n"
     ]
    }
   ],
   "source": [
    "# 時間計測開始\n",
    "time_sta = time.time()\n",
    "\n",
    "# 処理開始\n",
    "\n",
    "#前処理したデータ\n",
    "df_pret = rem_error(df_raw)\n",
    "\n",
    "#機械学習に使うデータ_新人用\n",
    "df_pret_shinjin = pret_data_shinjin(df_pret)\n",
    "\n",
    "#機械学習に使うデータ_経験者用\n",
    "df_pret_keiken = pret_data_keiken(df_pret)\n",
    "\n",
    "#機械学習に使うデータ_経験者だが過去データがない\n",
    "df_pret_etc = pret_data_etc(df_pret)\n",
    "\n",
    "# 時間計測終了\n",
    "time_end = time.time()\n",
    "# 経過時間（秒）\n",
    "tim = time_end- time_sta\n",
    "\n",
    "print(tim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861edeb-292f-4b94-831d-3afb47126c54",
   "metadata": {},
   "source": [
    "### データ数チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3dd96821-4f3b-45c1-9131-34b49c7ef9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4259 2363 748 1148\n"
     ]
    }
   ],
   "source": [
    "#新人\n",
    "df = df_pret_shinjin\n",
    "print(len(df),len(df[df['data']==0]),len(df[df['data']==1]),len(df[df['data']==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f981aba-38f6-4d73-8de3-3b46531ef2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7622 4186 1679 1757\n"
     ]
    }
   ],
   "source": [
    "#経験\n",
    "df = df_pret_keiken\n",
    "print(len(df),len(df[df['data']==0]),len(df[df['data']==1]),len(df[df['data']==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e039eff2-2eed-450c-ad47-5ea12d0d5b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969 929 152 888\n"
     ]
    }
   ],
   "source": [
    "#経験　過去データなし\n",
    "print(len(df),len(df[df['data']==0]),len(df[df['data']==1]),len(df[df['data']==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76012f1d-9119-4865-9ff1-920eee496dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Excelの書き出し \n",
    "\"\"\"\n",
    "name = \"採点者品質予測_22冬納品_前処理済み学習データ_v1\"\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/intermediatedata\"#フォルダパス\n",
    "\n",
    "#Excel\n",
    "with pd.ExcelWriter('{}/{}.xlsx'.format(path_folder,name)) as writer:\n",
    "    df_raw.to_excel(writer, sheet_name='整形済みデータ',encoding='utf-8-sig', index = False)\n",
    "    df_pret.to_excel(writer, sheet_name='前処理済みデータ',encoding='utf-8-sig', index = False)\n",
    "    df_pret_shinjin.to_excel(writer, sheet_name='機械学習用データ_新人',encoding='utf-8-sig', index = False)\n",
    "    df_pret_keiken.to_excel(writer, sheet_name='機械学習用データ_経験者',encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27cced3c-ad6f-47a6-aeba-a71e785a5f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新人のサンプル数4259\n"
     ]
    }
   ],
   "source": [
    "#N数の確認\n",
    "df = df_pret_shinjin\n",
    "df.dropna(subset=['偏差値'], inplace=True)\n",
    "print('新人のサンプル数{}'.format(str(len(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd2a46cc-f5e6-462c-be00-4d69fd07c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "経験者の参加回数別のサンプル数7622\n",
      "参加経験1回のサンプル数1416\n",
      "参加経験2回のサンプル数1133\n",
      "参加経験3回のサンプル数1057\n",
      "参加経験4回のサンプル数1040\n",
      "参加経験5回のサンプル数813\n",
      "参加経験6回以上のサンプル数2163\n"
     ]
    }
   ],
   "source": [
    "#N数の確認\n",
    "df = df_pret_keiken\n",
    "df.dropna(subset=['偏差値'], inplace=True)\n",
    "print('経験者の参加回数別のサンプル数{}'.format(str(len(df))))\n",
    "for i in range(1,6):\n",
    "    print('参加経験{}回のサンプル数{}'.format(str(i),str(len(df[df['今回の参加回数']==i]))))\n",
    "print('参加経験6回以上のサンプル数{}'.format(str(len(df[df['今回の参加回数']>=6]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b2992-7c3a-4c1b-bb34-9d7af3e17952",
   "metadata": {
    "tags": []
   },
   "source": [
    "## モデル作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ba39c-371a-4694-b818-82fe55995018",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DecisionTreeRegressorメソッドの概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3cf2cb-fe3d-40b2-9ca6-41fb64ec13fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "|引数名|概要|デフォルト|\n",
    "| :---- | :---- | :---- |\n",
    "|criterion|不純度を測定する基準（平均二乗誤差、平均絶対誤差など）|‘mse’|\n",
    "|splitter|条件探索アルゴリズムを選択するオプション（’best’と’rondom’が指定可能）|‘best’|\n",
    "|max_depth|決定木のノード深さの制限値。ツリーが深くなりすぎて過学習の状態に陥った際は、このパラメータが正則化の役割を果たす。|None|\n",
    "|min_samples_split|ノードを分割するために必要なサンプルの最小値|2|\n",
    "|min_samples_leaf|1ノードの深さを作成するために必要となるデータ数の最小値。指定した値以上のデータ数を持たないノードは作られない。|1|\n",
    "|min_weight_fraction_leaf|サンプルの重みを考慮した上でのmin_samples_leafに該当|0.0|\n",
    "|max_features|ランダムに指定する説明変数の数(全ての説明変数がモデル学習に活用されるわけではなく、ランダムに割り振られる）|None|\n",
    "|random_state|乱数シード|None|\n",
    "|max_leaf_nodes|作成される決定木の葉の数を、指定した値以下に制御する|None|\n",
    "|min_impurity_decrease|決定木の成長の早期停止するための閾値。不純度が指定の値より減少した場合、ノードを分岐し、不純度が指定の値より減少しなければ分岐を抑制。|0.0|\n",
    "|ccp_alpha|ccp_alphaが大きいほどプルーニングされるノードの数が増加。プルーニングとは、精度低下をできるだけ抑えながら過剰な重みを排除するプロセスを指す。|0.0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8461ead-2587-4b1c-ac3e-b4900e26f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://stackoverflow.com/questions/50607740/reverse-a-get-dummies-encoding-in-pandas\n",
    "ダミー変数を元に戻す。エクセル出力用\n",
    "\"\"\"\n",
    "\n",
    "def undummify(df, prefix_sep=\"_\"):\n",
    "    cols2collapse = {\n",
    "        item.split(prefix_sep)[0]: (prefix_sep in item) for item in df.columns\n",
    "    }\n",
    "    series_list = []\n",
    "    for col, needs_to_collapse in cols2collapse.items():\n",
    "        if needs_to_collapse:\n",
    "            undummified = (\n",
    "                df.filter(like=col)\n",
    "                .idxmax(axis=1)\n",
    "                .apply(lambda x: x.split(prefix_sep, maxsplit=1)[1])\n",
    "                .rename(col)\n",
    "            )\n",
    "            series_list.append(undummified)\n",
    "        else:\n",
    "            series_list.append(df[col])\n",
    "    undummified_df = pd.concat(series_list, axis=1)\n",
    "    return undummified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ac955a4-08a1-47ef-882a-e52546b30dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['スタッフコード', '参加回数', '科目', '分野', '年度', '採点回', '学年', '完全一致率',\n",
       "       '登録試験点数', '身分', '年齢', '偏差値', 'data'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#機械学習に使うデータの前処理_新人用\n",
    "df_pret_shinjin.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb928d90-f07f-4d13-a874-90ad2c59a412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_data_shinjin(df_input):\n",
    "    \n",
    "    \"\"\"\n",
    "    学習,検証,予測データを作成\n",
    "    1.データから不要な列を削除\n",
    "    2.ダミー化\n",
    "    3.説明変数と目的変数に分ける\n",
    "    4.学習と検証に分ける\n",
    "    5.選択用変数を除外し、各データフレームを作成\n",
    "    6.[X_train,y_train,X_test,y_test,X_test_info]をデータフレームとしてreturn\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    df_pred = df[df['data']==2].copy()\n",
    "    \n",
    "    #rem_y = '参加回数'\n",
    "    use_y = '完全一致率'\n",
    "\n",
    "    #1.不要な変数を削除\n",
    "    #df = df.drop(columns = rem_y)\n",
    "\n",
    "    #2.名義変数のエンコーディング pandas get_dummies関数でone hot encording\n",
    "    df = pd.get_dummies(df, columns=['分野','身分'])\n",
    "    \n",
    "    #3.上記のデータセットを説明変数と目的変数で分ける\n",
    "    df_X = df.drop(columns = use_y)#目的変数を除外した（説明変数と選択用変数だけ含む）データフレーム\n",
    "    df_y = df.loc[:,[use_y,'data']]#目的変数と選択用変数だけ含むデータフレーム　\n",
    "    \n",
    "    #4.データフレームの分離\n",
    "    \"\"\"秋冬が訓練用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_train = df_X[df_X['data']==0]\n",
    "    #訓練用目的変数\n",
    "    y_train = df_y[df_y['data']==0]\n",
    "\n",
    "    \"\"\"22夏が検証用\"\"\"\n",
    "    #説明変数\n",
    "    X_test = df_X[df_X['data']==1]\n",
    "    #目的変数\n",
    "    y_test = df_y[df_y['data']==1]\n",
    "\n",
    "    \"\"\"予測用\"\"\"\n",
    "    #説明変数\n",
    "    X_pred = df_X[df_X['data']==2]#秋に変更するところ\n",
    "    \n",
    "    #5.不要な変数を削除\n",
    "    rem_cols_x = [\"スタッフコード\",\"採点回\",\"年度\",\"科目\",\"参加回数\",\"data\"]\n",
    "    rem_cols_y = [\"data\"]\n",
    "\n",
    "    #説明変数\n",
    "    X_train = X_train.drop(columns = rem_cols_x)\n",
    "    X_test_id =X_test.loc[:,['スタッフコード']]\n",
    "    X_test_kamoku =X_test.loc[:,['科目']]\n",
    "    X_test = X_test.drop(columns = rem_cols_x)\n",
    "    X_pred = X_pred.drop(columns = rem_cols_x)\n",
    "         \n",
    "    #目的変数\n",
    "    y_train = y_train.drop(columns = rem_cols_y)\n",
    "    y_test = y_test.drop(columns = rem_cols_y)\n",
    "    \n",
    "    #各モデル用データを各データフレームにまとめ、それらをさらにデータフレームにまとめて返す。\n",
    "    df_res=[X_train,y_train,X_test,y_test,X_test_id,X_test_kamoku, X_pred, df_pred]\n",
    "\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "405dddd3-d46c-4416-bb8d-254fe2fb9bc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_data_shinjin(df_input):#オリジナル\n",
    "    \n",
    "    \"\"\"\n",
    "    学習,検証,予測データを作成\n",
    "    1.データから不要な列を削除\n",
    "    2.ダミー化\n",
    "    3.説明変数と目的変数に分ける\n",
    "    4.学習と検証に分ける\n",
    "    5.選択用変数を除外し、各データフレームを作成\n",
    "    6.[X_train,y_train,X_test,y_test,X_test_info]をデータフレームとしてreturn\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    #0の場合、完全一致率を予測。　1の場合、最終ペース相対値を予測\n",
    "    rem_y = '参加回数'\n",
    "    use_y = '完全一致率'\n",
    "\n",
    "    #1.不要な変数を削除\n",
    "    df = df.drop(columns = rem_y)\n",
    "\n",
    "    #2.名義変数のエンコーディング pandas get_dummies関数でone hot encording\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    #3.上記のデータセットを説明変数と目的変数で分ける\n",
    "    df_X = df.drop(columns = use_y)#目的変数を除外した（説明変数と選択用変数だけ含む）データフレーム\n",
    "    df_y = df.loc[:,[use_y,'data']]#目的変数と選択用変数だけ含むデータフレーム　\n",
    "    \n",
    "    #4.データフレームの分離\n",
    "    \"\"\"秋冬が訓練用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_train = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==2022))]#秋に変更するところ\n",
    "    #訓練用目的変数\n",
    "    y_train = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==2022))]#秋に変更するところ\n",
    "\n",
    "    \"\"\"22夏が検証用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_test = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==2022)]#秋に変更するところ\n",
    "    #訓練用目的変数\n",
    "    y_test = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==2022)]#秋に変更するところ\n",
    "    \n",
    "    #5.不要な変数を削除\n",
    "    rem_cols_x = [\"スタッフコード\",\"採点回\",\"年度\",\"科目\",\"参加回数\",\"data\"]\n",
    "    rem_cols_y = [\"data\"]\n",
    "\n",
    "    #説明変数\n",
    "    X_train = X_train.drop(columns = rem_cols_x)\n",
    "    X_test_id =X_test.loc[:,['スタッフコード']]\n",
    "    X_test_kamoku =X_test.loc[:,['科目']]\n",
    "    X_test = X_test.drop(columns = rem_cols_x)\n",
    "         \n",
    "    #目的変数\n",
    "    y_train = y_train.drop(columns = rem_cols_y)\n",
    "    y_test = y_test.drop(columns = rem_cols_y)\n",
    "    \n",
    "    #各モデル用データを各データフレームにまとめ、それらをさらにデータフレームにまとめて返す。\n",
    "    df_res=[X_train,y_train,X_test,y_test,X_test_id,X_test_kamoku]\n",
    "\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4154c4ca-5ccd-4571-b4ba-9e04ca1d8dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['スタッフコード', '前回の参加回数', '前回の分野', '前回の完全一致率', '科目', '今回の参加回数',\n",
       "       '今回の分野', '今回の年度', '今回の採点回', '学年', '今回の完全一致率', '登録試験点数', '身分', '年齢',\n",
       "       '偏差値', 'data'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pret_keiken.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "431a1fc5-6a5d-4f69-b173-b745fa863575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_data_keiken(df_input):\n",
    "    \n",
    "    \"\"\"\n",
    "    学習,検証,予測データを作成\n",
    "    1.データから不要な列を削除\n",
    "    2.ダミー化\n",
    "    3.説明変数と目的変数に分ける\n",
    "    4.学習と検証に分ける\n",
    "    5.選択用変数を除外し、各データフレームを作成\n",
    "    6.[X_train,y_train,X_test,y_test,X_test_info]をデータフレームとしてreturn\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    df_pred = df[df['data']==2].copy()\n",
    "    df_pred = df_pred.rename(columns = {'今回の採点回':'採点回',#0始まりで数えているため、\n",
    "                             '今回の年度':'年度',\n",
    "                             '今回の参加回数':'参加回数',\n",
    "                             '今回の分野':'分野'})\n",
    "    rem_y = ['前回の分野','前回の参加回数']\n",
    "    use_y = '今回の完全一致率'\n",
    "\n",
    "    \n",
    "    #1.不要な変数を削除\n",
    "    df = df.drop(columns = rem_y)\n",
    "    df = df.rename(columns = {'今回の採点回':'採点回',#0始まりで数えているため、\n",
    "                             '今回の年度':'年度',\n",
    "                             '今回の参加回数':'参加回数',\n",
    "                             '今回の分野':'分野'})\n",
    "\n",
    "    \n",
    "    #2.名義変数のエンコーディング pandas get_dummies関数でone hot encording\n",
    "    df = pd.get_dummies(df, columns=['分野','身分'])\n",
    "    \n",
    "    #3.上記のデータセットを説明変数と目的変数で分ける\n",
    "    df_X = df.drop(columns = use_y)#目的変数を除外した（説明変数と選択用変数だけ含む）データフレーム\n",
    "    df_y = df.loc[:,['data',use_y]]#目的変数と選択用変数だけ含むデータフレーム　#秋に変更するところ\n",
    "    \n",
    "    #4.データフレームの分離\n",
    "    \"\"\"秋冬が訓練用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_train = df_X[df_X['data']==0]#秋に変更するところ\n",
    "    #訓練用目的変数\n",
    "    y_train = df_y[df_y['data']==0]#秋に変更するところ\n",
    "\n",
    "    \"\"\"22夏が検証用\"\"\"\n",
    "    #説明変数\n",
    "    X_test = df_X[df_X['data']==1]#秋に変更するところ\n",
    "    #目的変数\n",
    "    y_test = df_y[df_y['data']==1]#秋に変更するところ\n",
    "\n",
    "    \"\"\"予測用\"\"\"\n",
    "    #説明変数\n",
    "    X_pred = df_X[df_X['data']==2]#秋に変更するところ\n",
    "    \n",
    "    #5.不要な変数を削除\n",
    "    rem_cols_x = [\"スタッフコード\",\"採点回\",\"年度\",\"科目\",\"data\"]\n",
    "    rem_cols_y = [\"data\"]#秋に変更するところ\n",
    "\n",
    "    #説明変数\n",
    "    X_train = X_train.drop(columns = rem_cols_x)\n",
    "    X_test_id =X_test.loc[:,['スタッフコード']]\n",
    "    X_test_kamoku =X_test.loc[:,['科目']]\n",
    "    X_test = X_test.drop(columns = rem_cols_x)\n",
    "    X_pred = X_pred.drop(columns = rem_cols_x)\n",
    "         \n",
    "    #目的変数\n",
    "    y_train = y_train.drop(columns = rem_cols_y)\n",
    "    y_test = y_test.drop(columns = rem_cols_y)\n",
    "    \n",
    "    #各モデル用データを各データフレームにまとめ、それらをさらにデータフレームにまとめて返す。\n",
    "    df_res=[X_train,y_train,X_test,y_test,X_test_id,X_test_kamoku, X_pred, df_pred]\n",
    "\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4950f28-8bf7-476b-a35f-5983dd437e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "モデル作成\n",
    "\"\"\"\n",
    "def train_model(df,a=2,b=1,c=None):\n",
    "    X_train= df[0]\n",
    "    y_train= df[1]\n",
    "    model = DecisionTreeRegressor(criterion='mse', \n",
    "                                   splitter='best', \n",
    "                                   max_depth=c, \n",
    "                                   min_samples_split=a, #3,4,5とか？\n",
    "                                   min_samples_leaf=b,#2とか \n",
    "                                   min_weight_fraction_leaf=0.0,\n",
    "                                   max_features=None, \n",
    "                                   random_state=0,#出力結果の固定のため \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   min_impurity_decrease=0.0, \n",
    "                                   ccp_alpha=0.0\n",
    "                                  )\n",
    "\n",
    "    #上記のパラメータでモデルを学習する\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5251fdc-bf56-484d-a1c7-f2ebbb6a6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(df,model):\n",
    "    \"\"\"\n",
    "    モデルを使って、予測値を出し、予測値、最終ペース、差分、乖離度（予測値/最終ペース）を列に追加したdfを返す。\n",
    "    \"\"\"\n",
    "    X_train= df[0]\n",
    "    y_train= df[1]\n",
    "    X_test = df[2]\n",
    "    y_test = df[3]\n",
    "    X_test_id = df[4]\n",
    "    X_test_kamoku = df[5]\n",
    "    y_pred  = model.predict(X_test)\n",
    "    \n",
    "    #得た結果を学習データとマージしてデータフレームで返す\n",
    "    \n",
    "    df_test=[]\n",
    "    df_test = undummify(X_test)  #企画ペースを入れるためにここをいじった。元はX_test\n",
    "    df_test.loc[:,'スタッフコード']= X_test_id\n",
    "    df_test.loc[:,'科目']= X_test_kamoku\n",
    "    df_test.loc[:,'完全一致率']= y_test\n",
    "    df_test.loc[:,'AI想定完全一致率']= y_pred #上のデータに予測値をマージ\n",
    "    #学習データの同じ学年分野カテゴリの中の基礎統計量\n",
    "    df_train = undummify(X_train)\n",
    "    df_train.loc[:,'完全一致率'] = y_train\n",
    "    df_temp = df_train.groupby(['分野','学年'],as_index=False).agg(\n",
    "        学習データ_N数=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"count\"),\n",
    "        学習データ_平均値=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"mean\"),\n",
    "        学習データ_中央値=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"median\"),\n",
    "        学習データ_最大値=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"max\"),\n",
    "        学習データ_最小値=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"min\"))\n",
    "    df_res = pd.merge(df_test, df_temp, on=['分野', '学年'], how='left')\n",
    "    # 実測値_完全一致率をランク分け\n",
    "    x = \"完全一致率\"\n",
    "    conditions = [\n",
    "            (df_res[x] >= 0.95),\n",
    "            (df_res[x] >= 0.7)\n",
    "             ]\n",
    "    choices = [\"0.95~\",\"0.70~0.95\"]\n",
    "    df_res.loc[:,'実測一致率ランク'] = np.select(conditions, choices, default = \"~0.70\")\n",
    "\n",
    "        # AI想定完全一致率をランク分け 評価用\n",
    "    x = \"AI想定完全一致率\"\n",
    "    conditions = [\n",
    "            (df_res[x] >= 0.95),\n",
    "            (df_res[x] >= 0.7)\n",
    "             ]\n",
    "    choices = [\"0.95~\",\"0.70~0.95\"]\n",
    "    df_res.loc[:,'想定一致率ランク'] = np.select(conditions, choices, default = \"~0.70\")\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c3809e7-8455-4654-8ace-c4b86ee484b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(df,model):\n",
    "    \"\"\"\n",
    "    モデルを使って、予測値を出し、予測値、最終ペース、差分、乖離度（予測値/最終ペース）を列に追加したdfを返す。\n",
    "    \"\"\"\n",
    "    X_train= df[0]\n",
    "    y_train= df[1]\n",
    "    X_test = df[2]\n",
    "    y_test = df[3]\n",
    "    X_test_id = df[4]\n",
    "    X_test_kamoku = df[5]\n",
    "    X_pred = df[6]\n",
    "    df_pred = df[7]\n",
    "    y_pred  = model.predict(X_pred)\n",
    "    \n",
    "    #得た結果を学習データとマージしてデータフレームで返す\n",
    "    \n",
    "    df_test=[]\n",
    "    df_test = undummify(df_pred)  #企画ペースを入れるためにここをいじった。元はX_test\n",
    "    df_test.loc[:,'AI想定完全一致率']= y_pred #上のデータに予測値をマージ\n",
    "    #学習データの同じ学年分野カテゴリの中の基礎統計量\n",
    "    df_train = undummify(X_train)\n",
    "    df_train.loc[:,'完全一致率'] = y_train\n",
    "    df_temp = df_train.groupby(['分野','学年'],as_index=False).agg(\n",
    "        学習データ_N数=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"count\"),\n",
    "        学習データ_平均値=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"mean\"),\n",
    "        学習データ_中央値=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"median\"),\n",
    "        学習データ_最大値=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"max\"),\n",
    "        学習データ_最小値=pd.NamedAgg(column=\"完全一致率\", aggfunc=\"min\"))\n",
    "    df_res = pd.merge(df_test, df_temp, on=['分野', '学年'], how='left')\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2065ac86-285d-4f08-9433-f451f6aa50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importance(df,model):\n",
    "    \"\"\"\n",
    "    変数の重要度を返す\n",
    "    \"\"\"\n",
    "    X_test = df[2]\n",
    "    feature = model.feature_importances_\n",
    "    label = X_test.columns\n",
    "    indices = np.argsort(feature)\n",
    "\n",
    "    # 特徴量の重要度の棒グラフ\n",
    "    fig =plt.figure (figsize = (10,10))\n",
    "    plt.ion()\n",
    "    plt.barh(range(len(feature)), feature[indices])\n",
    "    plt.yticks(range(len(feature)), label[indices], fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.ylabel(\"Feature\", fontsize=18)\n",
    "    plt.xlabel(\"Feature Importance\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c294e285-49bc-49dc-99f4-d36740eb750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用ライブラリ　樹形図作成\n",
    "# from sklearn import tree\n",
    "# import graphviz\n",
    "def make_treegraph(df,model):\n",
    "    \"\"\"\n",
    "    樹形図を作成し、返す\n",
    "    \"\"\"\n",
    "    X_train= df[0]\n",
    "    X_train_feature_names = X_train.columns.values.tolist()\n",
    "    dot_data = tree.export_graphviz(model, out_file=None,\n",
    "                                    feature_names= X_train_feature_names,\n",
    "                                    # class_names=iris.target_names,\n",
    "                                    filled=True, rounded=True, special_characters=True\n",
    "                                    )\n",
    "    treegraph = graphviz.Source(dot_data) \n",
    "    return treegraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381364fd-51c6-481e-bf02-6e529447ced9",
   "metadata": {},
   "source": [
    "## 採点者別品質予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d9250d5-f30d-46ff-a835-dd917af20cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#検証用\n",
    "def get_result_test(df_input,case=int):\n",
    "    df = df_input.copy()\n",
    "    df_temp=[]#[X_train,y_train,X_test,y_test,X_test_id,X_test_kamoku]\n",
    "    model = []\n",
    "    pred = []\n",
    "    #新人モデルの結果\n",
    "    if case == 0:\n",
    "        df_temp = make_data_shinjin(df)\n",
    "        model = train_model(df_temp,a=2,b=1,c=None)\n",
    "        pred = get_test(df_temp, model)\n",
    "    #経験者モデルの結果\n",
    "    elif case == 1:\n",
    "        df_temp = make_data_keiken(df)\n",
    "        model = train_model(df_temp,a=2,b=1,c=None)\n",
    "        pred = get_test(df_temp, model)\n",
    "            \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98667d08-278e-493d-9f01-0da445bf2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#予測用\n",
    "def get_result_pred(df_input,case=int):\n",
    "    df = df_input.copy()\n",
    "    df_temp=[]#[X_train,y_train,X_test,y_test,X_test_id,X_test_kamoku]\n",
    "    model = []\n",
    "    pred = []\n",
    "    #新人モデルの結果\n",
    "    if case == 0:\n",
    "        df_temp = make_data_shinjin(df)\n",
    "        model = train_model(df_temp,a=2,b=1,c=None)\n",
    "        pred = get_pred(df_temp, model)\n",
    "    #経験者モデルの結果\n",
    "    elif case == 1:\n",
    "        df_temp = make_data_keiken(df)\n",
    "        model = train_model(df_temp,a=2,b=1,c=None)\n",
    "        pred = get_pred(df_temp, model)\n",
    "            \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27282ebe-ce35-48d2-af8a-f39961d1fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果出力\n",
    "target_names = ['0.70~0.95', '~0.70', '0.95~']\n",
    "a=['実測一致率ランク','想定一致率ランク']#y_true, y_pred\n",
    "df1 = get_result_test(df_pret_shinjin, case = 0)#新人モデルの検証結果\n",
    "df2 = get_result_test(df_pret_keiken, case = 1)#経験モデルの検証結果\n",
    "df3 = get_result_test(df_pret_etc, case = 0)#経験（過去データなし）モデルの検証結果 新人モデルと同じ学習法で、データセットだけ変えてある\n",
    "\n",
    "df1_pred = get_result_pred(df_pret_shinjin, case = 0)#新人モデルの予測結果\n",
    "df2_pred = get_result_pred(df_pret_keiken, case = 1)#経験モデルの予測結果\n",
    "df3_pred = get_result_pred(df_pret_etc, case = 0)#経験（過去データなし）モデルの予測結果\n",
    "\n",
    "\n",
    "name_file = \"採点者品質予測_22冬_予測値_1227_v2\" #ファイル名\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/output\"#フォルダパス\n",
    "path_file = r'{p}/{n}'.format(p = path_folder, n = name_file)#ファイルパスとファイル名\n",
    "\n",
    "with pd.ExcelWriter('{}/{}.xlsx'.format(path_folder,name_file)) as writer:\n",
    "    df1_pred.to_excel(writer, sheet_name='予測_新人_分野',encoding='utf-8-sig', index = False)\n",
    "    df2_pred.to_excel(writer, sheet_name='予測_経験_分野',encoding='utf-8-sig', index = False)\n",
    "    df3_pred.to_excel(writer, sheet_name='予測_経験過去無し_分野',encoding='utf-8-sig', index = False)\n",
    "    df1.to_excel(writer, sheet_name='22夏検証_新人_分野',encoding='utf-8-sig', index = False)\n",
    "    df2.to_excel(writer, sheet_name='22夏検証_経験_分野',encoding='utf-8-sig', index = False)\n",
    "    df3.to_excel(writer, sheet_name='22夏検証_経験過去無し_分野',encoding='utf-8-sig', index = False)\n",
    "    pd.DataFrame(classification_report(y_true = list(df1[a[0]]),\n",
    "                                       y_pred = list(df1[a[1]]),\n",
    "                                       target_names=target_names,output_dict=True)).to_excel(writer, sheet_name='22夏検証_新人_分野_score',encoding='utf-8-sig', index = True)\n",
    "    pd.DataFrame(classification_report(y_true = list(df2[a[0]]),\n",
    "                                       y_pred = list(df2[a[1]]),\n",
    "                                       target_names=target_names,output_dict=True)).to_excel(writer, sheet_name='22夏検証_経験_分野_score',encoding='utf-8-sig', index = True)\n",
    "    pd.DataFrame(classification_report(y_true = list(df3[a[0]]),\n",
    "                                       y_pred = list(df3[a[1]]),\n",
    "                                       target_names=target_names,output_dict=True)).to_excel(writer, sheet_name='22夏検証_経験過去無し_分野_score',encoding='utf-8-sig', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60789128-8082-41b6-be27-2f514d82a857",
   "metadata": {},
   "source": [
    "##　予測値を納品用に整形"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd9804-e70b-4ed2-91f4-8c8d2ebf758c",
   "metadata": {},
   "source": [
    "#### 受領したデータを型にして、新人モデルと予測モデルの結果をスタッフコードで結合する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7270002-479e-40cc-8d8f-2a6a003f600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#整形済みデータの読み込み\n",
    "\n",
    "\"\"\"\n",
    "納品用に、受領データ（分野情報を補足したもの）を読み込む\n",
    "\"\"\"\n",
    "# \n",
    "path_name = \"/Users/s.ogura/Documents/CRLEA/data/intermediatedata/採点者品質予測_22冬納品_整形済み学習データ_参加回数修正済み_v1.xlsx\"\n",
    "path_file =  r'{}'.format(path_name)\n",
    "input_file = pd.ExcelFile(path_file)\n",
    "sn_list = input_file.sheet_names\n",
    "\n",
    "\n",
    "df = []\n",
    "df = pd.read_excel(path_file, sn_list[0], header=0, index_col=None,skiprows=None)\n",
    "#usecols=['スタッフコード', '年度', '採点回', '学年', '科目', '分野', '身分', '年齢','偏差値', '登録試験点数', '参加回数', '完全一致率', 'Error', 'data']\n",
    "df = df[df['data']==2]\n",
    "df_orijin = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f263cbac-62cd-4bca-aedb-79b850e42fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['スタッフコード', '年度', '採点回', '学年', '科目', '分野', '割当', '身分', '年齢', '大学',\n",
       "       '学部', '偏差値', '登録試験点数', '参加回数_修正前', '参加回数', '完全一致率', 'Error',\n",
       "       'data'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orijin.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14d2ddfd-dff0-4fb7-9048-cd6cc89d937a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['スタッフコード', '参加回数', '科目', '分野', '年度', '採点回', '学年', '完全一致率',\n",
       "       '登録試験点数', '身分', '年齢', '偏差値', 'data', 'AI想定完全一致率', '学習データ_N数',\n",
       "       '学習データ_平均値', '学習データ_中央値', '学習データ_最大値', '学習データ_最小値'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pred.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a561f9b-4cc5-4a9a-b002-bfbb1a0c5aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['スタッフコード', '前回の参加回数', '前回の分野', '前回の完全一致率', '科目', '参加回数', '分野',\n",
       "       '年度', '採点回', '学年', '今回の完全一致率', '登録試験点数', '身分', '年齢', '偏差値', 'data',\n",
       "       'AI想定完全一致率', '学習データ_N数', '学習データ_平均値', '学習データ_中央値', '学習データ_最大値',\n",
       "       '学習データ_最小値'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_pred.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c895d62-d4ff-480e-acb8-83d693362507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['スタッフコード', '参加回数', '科目', '分野', '年度', '採点回', '学年', '完全一致率',\n",
       "       '登録試験点数', '身分', '年齢', '偏差値', 'data', 'AI想定完全一致率', '学習データ_N数',\n",
       "       '学習データ_平均値', '学習データ_中央値', '学習データ_最大値', '学習データ_最小値'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_pred.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af5e2cbb-f17f-4fad-a923-feb3c0e07915",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df1_pred　新人モデルの予測結果\n",
    "df2_pred　経験モデルの予測結果\n",
    "df3_pred　経験モデルの予測結果\n",
    "各モデルの結合\n",
    "\"\"\"\n",
    "def select_res(df_input1, df_input2, df_input3):\n",
    "    # colname= ['スタッフコード', '参加回数', 'AI想定完全一致率', '学習データ_N数','学習データ_平均値', '学習データ_中央値', '学習データ_最大値', '学習データ_最小値']\n",
    "    colname= ['スタッフコード','AI想定完全一致率', '学習データ_N数','学習データ_平均値', '学習データ_中央値', '学習データ_最大値', '学習データ_最小値']\n",
    "    df1 = df_input1.copy()\n",
    "    df1 = df1[colname]\n",
    "    \n",
    "    df2 = df_input2.copy()\n",
    "    df2 = df2[colname]\n",
    "    \n",
    "    df3 = df_input3.copy()\n",
    "    df3 = df3[colname]\n",
    "    \n",
    "    df = df1.append(df2)\n",
    "    df = df.append(df3)\n",
    "    #df[colname] = df[colname].fillna('na')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_pred_comb = select_res(df1_pred,df2_pred,df3_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3cbc023-59fa-43b1-a5ac-d6c9d8dd74f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'受領件数': 3887, '予測件数': 3793, '欠損件数': 94}\n"
     ]
    }
   ],
   "source": [
    "num_check ={\"受領件数\":len(df_orijin),\n",
    " \"予測件数\":len(df_pred_comb),\n",
    " \"欠損件数\":len(df_orijin)-len(df_pred_comb)}\n",
    "print(num_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f12da0ce-0592-4075-b6c4-e58547b00592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1のキー重複\n",
      "0\n",
      "df2のキー重複\n",
      "0\n",
      "df1のオブザベーション:3887\n",
      "df2のオブザベーション:3793\n",
      "mergeのオブザベーション:3887\n"
     ]
    }
   ],
   "source": [
    "#受領した元データに予測値を結合\n",
    "df1 = df_orijin\n",
    "df2 = df_pred_comb\n",
    "df = pandas_tool.merge(df1,df2, key = [\"スタッフコード\"],how = 'left')\n",
    "colname= [ '参加回数','AI想定完全一致率', '学習データ_N数','学習データ_平均値', '学習データ_中央値', '学習データ_最大値', '学習データ_最小値']\n",
    "df[colname] = df[colname].fillna('na')\n",
    "collist= ['スタッフコード', '年度', '採点回', '学年', '科目', '分野', '割当', '身分', '年齢', '大学',\n",
    "       '学部', '偏差値', '登録試験点数', '参加回数_修正前', '参加回数', '完全一致率',\n",
    "         'AI想定完全一致率', '学習データ_N数', '学習データ_平均値',\n",
    "       '学習データ_中央値', '学習データ_最大値', '学習データ_最小値', \n",
    "        'Error','data', ]\n",
    "df=df[collist]\n",
    "df_output = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f2592d8-79fc-451c-87c4-7f6671362950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Excelの書き出し \n",
    "\"\"\"\n",
    "df = df_output\n",
    "\n",
    "name = \"採点者品質予測_22冬_納品_1227_v3\"\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/output\"#フォルダパス\n",
    "\n",
    "#Excel\n",
    "with pd.ExcelWriter('{}/{}.xlsx'.format(path_folder,name)) as writer:\n",
    "    df.to_excel(writer, sheet_name='予測値_採点者',encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086065c5-c592-492c-9743-42666ae08fba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ここまで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8250fde4-5d08-4d44-8aeb-ace7b5e3256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(df_input,case=int):\n",
    "    df = df_input.copy()\n",
    "    path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/output\"#フォルダパス\n",
    "    #新人モデルの結果\n",
    "    if case == 0:\n",
    "        df_temp= make_data_shinjin(df)\n",
    "        model = train_model(df_temp,a=2,b=1,c=None)\n",
    "        res = get_importance(df_temp, model)\n",
    "        name = \"採点者品質予測_22秋採点_重要度_新人_0906_v1\"#ファイル名\n",
    "        path_file = r'{p}/{n}'.format(p = path_folder, n = name)#ファイルパスとファイル名\n",
    "        res.savefig('{p}/{n}.png'.format(p = path_folder, n = name), format=\"png\", dpi=300)\n",
    "        treegraph = make_treegraph(df_temp, model)\n",
    "        name = \"採点者品質予測_22秋採点_樹形図_新人_0906_v1\".format(i)#ファイル名\n",
    "        path_file = r'{p}/{n}'.format(p = path_folder, n = name)#ファイルパスとファイル名\n",
    "        treegraph.render(path_file)\n",
    "\n",
    "    #経験者モデルの結果\n",
    "    elif case == 1:\n",
    "        df_temp= make_data_keiken(df)\n",
    "        model = train_model(df_temp,a=2,b=1,c=None)\n",
    "        res = get_importance(df_temp, model)\n",
    "        name = \"採点者品質予測_22秋採点_重要度_新人_0906_v1\"#ファイル名\n",
    "        path_file = r'{p}/{n}'.format(p = path_folder, n = name)#ファイルパスとファイル名\n",
    "        res.savefig('{p}/{n}.png'.format(p = path_folder, n = name), format=\"png\", dpi=300)\n",
    "        treegraph = make_treegraph(df_temp, model)\n",
    "        name = \"採点者品質予測_22秋採点_樹形図_新人_0906_v1\".format(i)#ファイル名\n",
    "        path_file = r'{p}/{n}'.format(p = path_folder, n = name)#ファイルパスとファイル名\n",
    "        treegraph.render(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d010c2e7-a07f-4d4a-9cea-6996055a5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#重要度、樹形図出力\n",
    "get_graph(df_pret_shinjin,case=0)\n",
    "get_graph(df_pret_keiken,case=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa55b34-7061-4914-b36a-2ffdf167eae1",
   "metadata": {},
   "source": [
    "# メモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c19b45de-a2c7-43c9-8189-abcdad2651ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#クラス分類評価指標　作成メモ\n",
    "from sklearn.metrics import classification_report\n",
    "df=df1.copy()\n",
    "\n",
    "#y_true = list(df[0]['実測一致率ランク'])#それぞれのランク\n",
    "#y_pred = list(df[0]['想定一致率ランク2'])#それぞれのランク\n",
    "\n",
    "target_names = ['0.70~0.95', '~0.70', '0.95~']\n",
    "d = pd.DataFrame(classification_report(y_true = list(df[0]['実測一致率ランク']), y_pred = list(df[0]['想定一致率ランク2']),\n",
    "                                       target_names=target_names,output_dict=True))\n",
    "\n",
    "#print(classification_report(y_true, y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b1632fa0-1693-4f7c-8bab-2a67c6be0c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果出力\n",
    "select_cols = ['AI想定完全一致率','完全一致率','実予誤差']\n",
    "target_names = ['0.70~0.95', '~0.70', '0.95~']\n",
    "a=['実測一致率ランク','想定一致率ランク2']#y_true, y_pred\n",
    "df1 = get_result(df_pret_shinjin, case = 0)#新人モデルの予測結果\n",
    "df2 = get_result(df_pret_keiken, case = 1)#経験モデルの予測結果\n",
    "\n",
    "\n",
    "\n",
    "name_file = \"採点者品質予測_科目混合_完全一致率_0902_v2\" #ファイル名\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/output\"#フォルダパス\n",
    "path_file = r'{p}/{n}'.format(p = path_folder, n = name_file)#ファイルパスとファイル名\n",
    "\n",
    "with pd.ExcelWriter('{}/{}.xlsx'.format(path_folder,name_file)) as writer:\n",
    "    df1[0].to_excel(writer, sheet_name='value_新人_分野',encoding='utf-8-sig', index = False)\n",
    "    df1[1].to_excel(writer, sheet_name='value_新人_科目',encoding='utf-8-sig', index = False)\n",
    "    df1[2].to_excel(writer, sheet_name='value_新人',encoding='utf-8-sig', index = False)\n",
    "    df2[0].to_excel(writer, sheet_name='value_経験_分野',encoding='utf-8-sig', index = False)\n",
    "    df2[1].to_excel(writer, sheet_name='value_経験_科目',encoding='utf-8-sig', index = False)\n",
    "    df2[2].to_excel(writer, sheet_name='value_経験',encoding='utf-8-sig', index = False)\n",
    "    pandas_tool.summary(df1[0].loc[:,select_cols])[0].to_excel(writer, sheet_name='summary_新人_分野',encoding='utf-8-sig', index = True)\n",
    "    pandas_tool.summary(df1[1].loc[:,select_cols])[0].to_excel(writer, sheet_name='summary_新人_科目',encoding='utf-8-sig', index = True)\n",
    "    pandas_tool.summary(df1[2].loc[:,select_cols])[0].to_excel(writer, sheet_name='summary_新人',encoding='utf-8-sig', index = True)\n",
    "    pandas_tool.summary(df2[0].loc[:,select_cols])[0].to_excel(writer, sheet_name='summary_経験_分野',encoding='utf-8-sig', index = True)\n",
    "    pandas_tool.summary(df2[1].loc[:,select_cols])[0].to_excel(writer, sheet_name='summary_経験_科目',encoding='utf-8-sig', index = True)\n",
    "    pandas_tool.summary(df2[2].loc[:,select_cols])[0].to_excel(writer, sheet_name='summary_経験',encoding='utf-8-sig', index = True)\n",
    "    pd.DataFrame(classification_report(y_true = list(df1[0][a[0]]),\n",
    "                                       y_pred = list(df1[0][a[1]]),\n",
    "                                       target_names=target_names,output_dict=True)).to_excel(writer, sheet_name='score_新人_分野',encoding='utf-8-sig', index = True)\n",
    "    pd.DataFrame(classification_report(y_true = list(df1[1][a[0]]),\n",
    "                                       y_pred = list(df1[1][a[1]]),\n",
    "                                       target_names=target_names,output_dict=True)).to_excel(writer, sheet_name='score_新人_科目',encoding='utf-8-sig', index = True)\n",
    "    pd.DataFrame(classification_report(y_true = list(df1[2][a[0]]),\n",
    "                                       y_pred = list(df1[2][a[1]]),\n",
    "                                       target_names=target_names,output_dict=True)).to_excel(writer, sheet_name='score_新人',encoding='utf-8-sig', index = True)\n",
    "    pd.DataFrame(classification_report(y_true = list(df2[0][a[0]]),\n",
    "                                       y_pred = list(df2[0][a[1]]),\n",
    "                                       target_names=target_names,output_dict=True)).to_excel(writer, sheet_name='score_経験_分野',encoding='utf-8-sig', index = True)\n",
    "    pd.DataFrame(classification_report(y_true = list(df2[1][a[0]]),\n",
    "                                       y_pred = list(df2[1][a[1]]),\n",
    "                                       target_names=target_names,output_dict=True)).to_excel(writer, sheet_name='score_経験_科目',encoding='utf-8-sig', index = True)\n",
    "    pd.DataFrame(classification_report(y_true = list(df2[2][a[0]]),\n",
    "                                       y_pred = list(df2[2][a[1]]),\n",
    "                                       target_names=target_names,output_dict=True)).to_excel(writer, sheet_name='score_経験',encoding='utf-8-sig', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80ceb724-b2eb-48f8-ba49-89a12bb5a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(df_input,case=int):\n",
    "    df = df_input.copy()\n",
    "    path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/output\"#フォルダパス\n",
    "    #新人モデルの結果\n",
    "    if case == 0:\n",
    "        dfs=[[],[],[]]\n",
    "        dfs[0] = make_data_shinjin(df,val=0)\n",
    "        dfs[1] = make_data_shinjin2(df,val=0)\n",
    "        dfs[2] = make_data_shinjin3(df,val=0)\n",
    "        models = [[],[],[]]\n",
    "        for i in range(0,len(dfs)):\n",
    "            model = train_model(dfs[i],a=2,b=1,c=None)\n",
    "            res = get_importance(dfs[i], model)\n",
    "            name = \"採点者品質予測_科目混合_完全一致率_重要度_新人{}_0902_v1\".format(i)#ファイル名\n",
    "            path_file = r'{p}/{n}'.format(p = path_folder, n = name)#ファイルパスとファイル名\n",
    "            res.savefig('{p}/{n}.png'.format(p = path_folder, n = name), format=\"png\", dpi=300)\n",
    "            treegraph = make_treegraph(dfs[i], model)\n",
    "            name = \"採点者品質予測_科目混合_完全一致率_樹形図_新人{}_0902_v1\".format(i)#ファイル名\n",
    "            path_file = r'{p}/{n}'.format(p = path_folder, n = name)#ファイルパスとファイル名\n",
    "            treegraph.render(path_file)\n",
    "\n",
    "    #経験者モデルの結果\n",
    "    elif case == 1:\n",
    "        dfs=[[],[],[]]\n",
    "        dfs[0] = make_data_keiken(df,val=0)\n",
    "        dfs[1] = make_data_keiken2(df,val=0)\n",
    "        dfs[2] = make_data_keiken3(df,val=0)\n",
    "        models = [[],[],[]]\n",
    "        for i in range(0,len(dfs)):\n",
    "            model = train_model(dfs[i],a=2,b=1,c=None)\n",
    "            res = get_importance(dfs[i], model)\n",
    "            name = \"採点者品質予測_科目混合_完全一致率_重要度_経験{}_0902_v1\".format(i)#ファイル名\n",
    "            path_file = r'{p}/{n}'.format(p = path_folder, n = name)#ファイルパスとファイル名\n",
    "            res.savefig('{p}/{n}.png'.format(p = path_folder, n = name), format=\"png\", dpi=300)\n",
    "            treegraph = make_treegraph(dfs[i], model)\n",
    "            name = \"採点者品質予測_科目混合_完全一致率_樹形図_経験{}_0902_v1\".format(i)#ファイル名\n",
    "            path_file = r'{p}/{n}'.format(p = path_folder, n = name)#ファイルパスとファイル名\n",
    "            treegraph.render(path_file)\n",
    "            \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
