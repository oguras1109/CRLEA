{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed4e5c2-5cc2-4182-912b-59b5c3ff5dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 作業工程計画\n",
    "\n",
    "0. ライブラリを読み込む\n",
    "1. 過去データを読み込む\n",
    "2. 過去データを整形する\n",
    "3. 過去データを学習してモデルを作る \n",
    "4. 設問データを読み込む\n",
    "5. 設問データをモデルに読み込ませ、予測値を出す\n",
    "6. 予測値を設問データに追加する\n",
    "7. 6のデータに、分野別最終ペースの最大値、最小値、平均値を追加する\n",
    "8. データを出力し、納品する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a61eb-834f-4150-a979-2b7ffe145fa6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a825b8e5-4df0-4c29-867e-f1e72ee3efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "import re \n",
    "\n",
    "\"\"\"\n",
    "機械学習ライブラリの準備\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor #決定木\n",
    "import lightgbm as lgb #lightGBM\n",
    "\n",
    "from sklearn import tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684e220-ecc1-4b36-bd44-bc4d08487d91",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 村上さん pandas_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edeb5627-7dea-45f6-b814-273133437f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "村上さんtoolbox\n",
    "\"\"\"\n",
    "# pandas 基礎集計クラス\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "#http://qiita.com/tanemaki/items/2ed05e258ef4c9e6caac\n",
    "\n",
    "# Jupyterで表示するためには、最初に以下を実行すること\n",
    "%matplotlib inline\n",
    "\n",
    "# Static Classで設計する\n",
    "class pandas_tool:\n",
    "    \n",
    "    # All in one チェック（Jupyterのみ）\n",
    "    def all_basic_summary_jupyter(df):\n",
    "        print(\"■ 型の確認\")\n",
    "        display(pandas_tool.type(df))\n",
    "        print(\"■ 数値型の確認\")\n",
    "        display(pandas_tool.summary(df)[0])\n",
    "        print(\"■ カテゴリ型の確認\")\n",
    "        cate_var_data = list(df.select_dtypes(include=['object']).columns)\n",
    "        ret = pandas_tool.freq(df,cate_var_data)\n",
    "        for d in ret:\n",
    "            display(pd.DataFrame(d))\n",
    "            print(\"---------------\")\n",
    "        print(\"■ 欠損の確認\")\n",
    "        display(pandas_tool.check_missing(df))\n",
    "    \n",
    "    # 相関関係可視化（Jupyterのみ）\n",
    "    def all_value_relation_visualize(df):\n",
    "        #sns.set_context(\"poster\", 1.2, {\"lines.linewidth\": 3})\n",
    "        sns.pairplot(df,size=5)\n",
    "    \n",
    "    # カテゴリ変数でのヒートマップ（Jupyterのみ）\n",
    "    def make_heatmap(df,x,y,value):\n",
    "        target_df = df.pivot_table(index=x,values=value,columns=y)\n",
    "        sns.heatmap(target_df, annot=True, fmt='1.1f', cmap='Blues')\n",
    "    \n",
    "    # 散布図（Jupyterのみ）\n",
    "    def make_scatter_chart(df,x,y):\n",
    "        #sns.jointplot(x=x, y=y, data=df, kind=\"hex\")\n",
    "        sns.jointplot(x=x, y=y, data=df)\n",
    "    \n",
    "    # 組み合わせでヒートマップを作成（Jupyterのみ）\n",
    "    def all_make_heatmap(df,var_list,value):\n",
    "        col_num = 2\n",
    "        var_list_set = list(combinations(var_list,2))\n",
    "        \n",
    "        fig, axes = plt.subplots(int(len(var_list_set)/col_num)+1, col_num, figsize=(18,3+6.5*int(len(var_list_set)/col_num)))\n",
    "        \n",
    "        for i,target in enumerate(var_list_set):\n",
    "            target_df = df.pivot_table(index=target[0],values=value,columns=target[1])\n",
    "            sns.heatmap(target_df, annot=True, fmt='1.1f', cmap='Blues', ax=axes[int(i/col_num), i%col_num])\n",
    "            \n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # 数値集計\n",
    "    def summary(df,view=False):\n",
    "        ret=df.describe()\n",
    "        mis_ret=df.isnull().sum()\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(\"・統計量\")\n",
    "            print(ret)\n",
    "            print(\"・欠損値\")\n",
    "            print(mis_ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        return ret,mis_ret\n",
    "    \n",
    "    # 型チェック\n",
    "    def type(df,view=False):\n",
    "        ret = df.dtypes\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_rows\")\n",
    "            pd.set_option(\"display.max_rows\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_rows\",param)\n",
    "        return ret\n",
    "    \n",
    "    # 欠損チェック\n",
    "    def check_missing(df,view=False):\n",
    "        not_null_df=df.notnull()\n",
    "        ret=pd.DataFrame()\n",
    "        for name in not_null_df.columns:\n",
    "            tmp_df=not_null_df[name].value_counts()\n",
    "            tmp_df.name=name\n",
    "            ret = pd.concat([ret,tmp_df],axis=1)\n",
    "        \n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # 欠損値のオブザベーションを抽出\n",
    "    def get_miss_data(df,column,view=False):\n",
    "        ret=df[df[column].isnull()]\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        return ret\n",
    "    \n",
    "    # 欠損値を中央値で補完\n",
    "    def fill_miss_med(df,var_name):\n",
    "        var=df[var_name].median()\n",
    "        df[var_name].fillna(var,inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # 欠損値を0で補完\n",
    "    def fill_miss_zero(df,var_name):\n",
    "        df[var_name].fillna(0,inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # 特定の値を欠損とみなす\n",
    "    def apply_miss_value(df,var_name,value):\n",
    "        df[var_name]=df[var_name].replace(value,np.nan)\n",
    "        return df\n",
    "    \n",
    "    # 重複チェック\n",
    "    def check_dup(df,columns,view=False):\n",
    "        ret=pd.DataFrame()\n",
    "        for name in columns:\n",
    "            dup_cnt=df[name].duplicated().sum()\n",
    "            tmp_df = pd.DataFrame({'var_name':[name],'dup_cnt':[dup_cnt]})\n",
    "            ret = pd.concat([ret,tmp_df],axis=0,ignore_index= True)\n",
    "        \n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # 組み合わせ重複チェック\n",
    "    def check_dup_comb(df,columns,view=False):\n",
    "        ret = df[columns].duplicated().sum()\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # ユニークデータ取得\n",
    "    def get_uniq_data(df,uniq_key,sort_key,keep='first'):\n",
    "        ret = df.sort_values(by=sort_key)\n",
    "        ret.drop_duplicates(subset=uniq_key, keep=keep, inplace=True)\n",
    "        return ret\n",
    "    \n",
    "    # カテゴリ集計\n",
    "    def freq(df,columns,view=False):\n",
    "        ret=list()\n",
    "        for name in columns:\n",
    "            tmp_df=df[name].value_counts()\n",
    "            tmp_df.name=name\n",
    "            #ret = pd.concat([ret,tmp_df],axis=1)\n",
    "            ret.append(tmp_df)\n",
    "        \n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            for r in ret:\n",
    "                print(r)\n",
    "                #display(r)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # 複雑な集計\n",
    "    def tabulate(df,row,col=None,var='',func=np.sum,view=False):\n",
    "        if var == '':\n",
    "            tmp_df=df.reset_index(drop=False,inplace=False)\n",
    "            ret=pd.pivot_table(data=tmp_df, values='index', index=row, columns=col, aggfunc='count', dropna=False, fill_value=0 ,margins = False)\n",
    "            tmp_df=None\n",
    "        else:\n",
    "            ret=pd.pivot_table(data=df, values=var, index=row, columns=col, aggfunc=func, dropna=False, fill_value=0 ,margins = False)\n",
    "        if view:\n",
    "            param=pd.get_option(\"display.max_columns\")\n",
    "            pd.set_option(\"display.max_columns\",1000)\n",
    "            print(ret)\n",
    "            pd.set_option(\"display.max_columns\",param)\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # マージ\n",
    "    def merge(df1,df2,key,how,view=True):\n",
    "        if view:\n",
    "            print(\"df1のキー重複\")\n",
    "            pandas_tool.check_dup_comb(df1,key,True)\n",
    "            print(\"df2のキー重複\")\n",
    "            pandas_tool.check_dup_comb(df2,key,True)\n",
    "            \n",
    "            print(\"df1のオブザベーション:{0}\".format(len(df1)))\n",
    "            print(\"df2のオブザベーション:{0}\".format(len(df2)))\n",
    "        \n",
    "        ret=pd.merge(df1,df2,how=how,on=key)\n",
    "        \n",
    "        if view:\n",
    "            print(\"mergeのオブザベーション:{0}\".format(len(ret)))\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # Rank\n",
    "    def rank(df,var,num,suffix='_rank',check=False):\n",
    "        labels=[i for i in range(0,num)]\n",
    "        df[var+suffix]=pd.qcut(df[var], num, labels=labels)\n",
    "        \n",
    "        # check data\n",
    "        if check:\n",
    "            ret=pd.DataFrame()\n",
    "            max_df=pandas_tool.tabulate(df=df,row=[var+suffix],var=var,func=np.max,view=False)\n",
    "            max_df.name='max'\n",
    "            min_df=pandas_tool.tabulate(df=df,row=[var+suffix],var=var,func=np.min,view=False)\n",
    "            min_df.name='min'\n",
    "            cnt_df=pandas_tool.tabulate(df=df,row=[var+suffix],var=var,func='count',view=False)\n",
    "            cnt_df.name='count'\n",
    "            ret=pd.concat([ret,min_df,max_df,cnt_df],axis=1)\n",
    "            return df,ret\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    # Rank適用(min基準)\n",
    "    def apply_rank(df,rank_df):\n",
    "        tmp_df=copy.deepcopy(rank_df)\n",
    "        tmp_df.reset_index(drop=False,inplace=True)\n",
    "        target_name=tmp_df.columns[3]\n",
    "        tmp_df.columns=[\"rank\",\"min\",\"max\",\"cnt\"]\n",
    "        \n",
    "        def judge_thld(row):\n",
    "            ret_var = -1\n",
    "            cond_list = [\"if 0 : ret_var = 0\"]\n",
    "            \n",
    "            for i in range(1,len(tmp_df)):\n",
    "                cond_list.append(\"elif row < \" +str(tmp_df.ix[i,'min'])+ \" : ret_var = \" + str(tmp_df.ix[i-1,'rank']))\n",
    "            \n",
    "            cond_list.append(\"else: ret_var = \" + str(tmp_df.ix[len(tmp_df)-1,'rank']))\n",
    "            cond_str=\"\\r\\n\".join(cond_list)\n",
    "            # ローカル辞書をexecと共有する\n",
    "            local_dict=locals()\n",
    "            exec(cond_str,local_dict)\n",
    "            return local_dict[\"ret_var\"]\n",
    "        \n",
    "        df[target_name+\"_rank\"]=df[target_name].apply(judge_thld)\n",
    "        return df\n",
    "    \n",
    "    # Min%以下はMin%点に、Max%以上はMax%点にクリップする\n",
    "    def clip_min_max(df,col_list,apply_df=None,max_pct=0.99,min_pct=0.01):\n",
    "        p_min = df[col_list].quantile(min_pct)\n",
    "        p_max = df[col_list].quantile(max_pct)\n",
    "        \n",
    "        df[col] = df[col_list].clip(p_min,p_max,axis=1)\n",
    "        \n",
    "        # もしも適用先のデータがあるならば（例えば検証データ）対応\n",
    "        if apply_df is not None:\n",
    "            apply_df[col] = apply_df[col_list].clip(p_min,p_max,axis=1)\n",
    "            return df,apply_df\n",
    "        else:\n",
    "            return df\n",
    "    \n",
    "    \n",
    "    # 文字列→数値変換\n",
    "    def conv_float(df,column,percent_flg=False):\n",
    "        \n",
    "        def conv_f(row):\n",
    "            if row[column] == \"\" or row[column] is np.nan:\n",
    "                return np.nan\n",
    "            else:\n",
    "                return float(row[column])\n",
    "        \n",
    "        df[column]=df[column].str.replace(\"\\\\\",\"\").str.replace(\",\",\"\").str.replace(\"%\",\"\").str.strip()\n",
    "        df[column]=df.apply(conv_f,axis=1)\n",
    "        \n",
    "        if percent_flg:\n",
    "            df[column]=df[column]/100\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e8ee8-5729-48bf-ad0d-944ea382a941",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 過去データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "30c7e7ba-881a-48b4-abe1-fb54ccea1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "path_name = ファイルパス\n",
    "sn_list = エクセルシート名のリスト\n",
    "\"\"\"\n",
    "path_name = \"/Users/s.ogura/Documents/CRLEA/data/rawdata/①設問情報（分野、文字数等の情報、企画ペース）_0522更新_欠損値ハイライト2_test.xlsx\"\n",
    "path_file =  r'{}'.format(path_name)\n",
    "input_file = pd.ExcelFile(path_file)\n",
    "sn_list = input_file.sheet_names\n",
    "#配列ループ\n",
    "df = [] #空のリスト\n",
    "for i in (0,1): #21冬\n",
    "\n",
    "    # Excelファイルの読み込み\n",
    "    ret = pd.read_excel(path_file, sn_list[i], header=0, index_col=None,skiprows=[1])#headerを修正\n",
    "    ret = ret.rename(columns={'企画ペース\\n完成版': '企画ペース',\n",
    "                             '文字数\\nＳ枠数': '文字数',\n",
    "                             'S枠（数字）': '割当ごとのS枠数',\n",
    "                             '割当単位':'割当'})#冬だけ\n",
    "    ret[\"年度\"]= sn_list[i][0:2]#sn_listのiつ目の要素から3番目の文字\n",
    "    ret[\"採点回\"]= sn_list[i][2]#sn_listのiつ目の要素から3番目の文字\n",
    "    ret[\"学年\"]= sn_list[i][3]#sn_listのiつ目の要素から4番目の文字    \n",
    "    # Excel sheetsを空のリストの各要素に追加\n",
    "    df.append(ret)\n",
    "    \n",
    "for i in (2,3,4): #21秋\n",
    "    # Excelファイルの読み込み\n",
    "    ret = pd.read_excel(path_file, sn_list[i], header=4,skiprows=[5], index_col=None)#headerを修正　,skiprows=[0,1,2,3,5]\n",
    "    ret[\"年度\"]= sn_list[i][0:2]#sn_listのiつ目の要素から3番目の文字\n",
    "    ret[\"採点回\"]= sn_list[i][2]#sn_listのiつ目の要素から3番目の文字\n",
    "    ret[\"学年\"]= sn_list[i][3]#sn_listのiつ目の要素から4番目の文字    \n",
    "    # Excel sheetsを空のリストの各要素に追加\n",
    "    df.append(ret)    \n",
    "\n",
    "for i in (5,6): #21夏1〜2\n",
    "    # Excelファイルの読み込み\n",
    "    ret = pd.read_excel(path_file, sn_list[i],header = 1, index_col=None)#headerを修正\n",
    "    ret[\"年度\"]= sn_list[i][0:2]#sn_listのiつ目の要素から3番目の文字\n",
    "    ret[\"採点回\"]= sn_list[i][2]#sn_listのiつ目の要素から3番目の文字\n",
    "    ret[\"学年\"]= sn_list[i][3]#sn_listのiつ目の要素から4番目の文字    \n",
    "    # Excel sheetsを空のリストの各要素に追加\n",
    "    df.append(ret)\n",
    "    \n",
    "for i in range(7,len(sn_list)): #21夏3〜\n",
    "    # Excelファイルの読み込み\n",
    "    ret = pd.read_excel(path_file, sn_list[i],header = 1, index_col=None)#headerを修正\n",
    "    ret[\"年度\"]= sn_list[i][0:2]#sn_listのiつ目の要素から3番目の文字\n",
    "    ret[\"採点回\"]= sn_list[i][2]#sn_listのiつ目の要素から3番目の文字\n",
    "    ret[\"学年\"]= sn_list[i][3]#sn_listのiつ目の要素から4番目の文字    \n",
    "    # Excel sheetsを空のリストの各要素に追加\n",
    "    df.append(ret)\n",
    "\n",
    "\"\"\"集計したデータを縦に結合する\"\"\"\n",
    "\n",
    "dfs = df[0] #集計結果を一つのDFに結合\n",
    "for i in df[1:]:\n",
    "    dfs = dfs.append(i)\n",
    "\n",
    "# rename    \n",
    "dfs = dfs.rename(columns={'科目分類名': '科目',\n",
    "                         '割当':'割当1'})\n",
    "\n",
    "#データ型の変更\n",
    "dfs['割当1'] = dfs.loc[:, \"割当1\"].astype('str')#下のfor文でKを除くために文字列型の必要あり\n",
    "dfs['学年'] = dfs.loc[:, \"学年\"].astype('int')\n",
    "dfs['年度'] = dfs.loc[:, \"年度\"].astype('int')\n",
    "\n",
    "\n",
    "#置換\n",
    "map_dictionary ={\"国語\" : 10, \"数学\" :20, \"英語\" : 30} \n",
    "dfs['科目コード'] = dfs['科目'].map(map_dictionary) \n",
    "\n",
    "dfs = dfs.replace({'最終ペース': {'ー':np.nan}})\n",
    "dfs['最終ペース'] = dfs.loc[:, \"最終ペース\"].astype('float64')\n",
    "\n",
    "#空白行の削除\n",
    "dfs= dfs[~dfs[\"科目コード\"].isna()]\n",
    "\n",
    "#割当から客観とnanを削除\n",
    "df = []\n",
    "for i in dfs['割当1']:\n",
    "    if 'K' in i:\n",
    "        i = np.nan\n",
    "    df.append(i)\n",
    "dfs['割当1'] = df\n",
    "dfs =dfs[dfs['割当1'] != \"nan\"]\n",
    "dfs= dfs[~dfs[\"割当1\"].isna()]\n",
    "\n",
    "df_1 = dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec2d279d-b29f-4ee4-9d63-fcd42f3f1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分野マスターは分野の表記揺れを統一したもの。 CRLEA様から修正版が送られてきたら、csvにして再度使用\n",
    "\"\"\"\n",
    "data1から分野を抜き出したcsvファイルを読み込む\n",
    "2つのテーブルを結合させる\n",
    "df_1:過去データを読み込んで一つのデータフレームにしたもの\n",
    "bunya:分野の表記揺れをなおした列（分野名（修正））がある\n",
    "\"\"\"\n",
    "\n",
    "name_csv = \"bunyamaster_v3.1.csv\"\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/rawdata\"#Excelが置いてあるフォルダパス\n",
    "path_file = r'{p}/{n}'.format(p = path_folder, n = name_csv)#ファイルパスとファイル名\n",
    "\n",
    "# csvファイルの読み込みと空のリストに追加\n",
    "bunya = pd.read_csv(filepath_or_buffer = path_file, sep=\",\")\n",
    "bunya = bunya.rename(columns={'分野名':'分野'})\n",
    "bunya = bunya.fillna({'解答言語': '日本語'})\n",
    "# merge\n",
    "df_1b = pd.merge(df_1,bunya, on = [\"分野\",\"科目コード\"],how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d2ac6937-7e4e-4b21-938b-439043265aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/_gcfyf2137x3x6dwgb9nkj740000gp/T/ipykernel_7660/139618028.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ret[j] =ret[j].fillna(ret[j].mean())\n"
     ]
    }
   ],
   "source": [
    "df = df_1b\n",
    "\n",
    "\"\"\"文字数　あとで数学の文字数を編集する必要あり。\"\"\"\n",
    "\n",
    "def make_sum(input_str):\n",
    "    if input_str ==\"\":\n",
    "        return np.nan\n",
    "    else:\n",
    "        input_list = input_str.split('＋')#+が半角か全角か異なる場合もあり。\n",
    "        input_list_num = [int(s) for s in input_list] #リスト内包表記　復習\n",
    "        return sum(input_list_num)\n",
    "\n",
    "\n",
    "#一行に対するルールを定義する 上から順に作成して、順次テストする\n",
    "x = ['文字程度','文字（説明）','字程度（日本語）','文字','※解答枠６。まとめて10点', '単語？','太枠', 'ポ採\\u30001枠', 'ポ採\\u30003枠','\\u3000','小枠2つ','2枠', '1枠', '3枠']\n",
    "y = [\"語程度\",\"語\"]\n",
    "z = ['5語程度×2枠', '50字以上60字以内（日本語）','30字以上40字以内']\n",
    "def make_mojisu(df):\n",
    "    \n",
    "    #行を含む場合は行数の文字列を代入\n",
    "    if '行' in str(df['文字数']):\n",
    "        return df['行の文字数']\n",
    "    \n",
    "    #文字数に数値が入っている場合、そのまま代入\n",
    "    elif str(df['文字数']).isdigit() == True: #簡単な例から場合分けする。 文字列か、数値か判定\n",
    "        return int(df['文字数'])\n",
    "\n",
    "    #ルール分けが難しい場合は直接数字で返す\n",
    "    if '5語程度×2枠' in str(df['文字数']):#ここも語数だから直す\n",
    "        return int(df['文字数'].replace('5語程度×2枠','10'))\n",
    "    if '50字以上60字以内（日本語）' in str(df['文字数']):\n",
    "        return int(df['文字数'].replace('50字以上60字以内（日本語）','50'))\n",
    "    if '30字以上40字以内' in str(df['文字数']):\n",
    "        return int(df['文字数'].replace('30字以上40字以内','30'))\n",
    "    if '30～45語' in str(df['文字数']):#ここも語数だから直す\n",
    "        return int(df['文字数'].replace('30～45語','30'))\n",
    "    if '5～10語' in str(df['文字数']):#ここも語数だから直す\n",
    "        return int(df['文字数'].replace('5～10語','5'))\n",
    "\n",
    "    #数値に文字列を含む場合は文字列を除外し数値だけ返す。また意味不明は空白で返す\n",
    "    for i in x:\n",
    "        if i in str(df['文字数']):\n",
    "            return make_sum(df['文字数'].replace(i,\"\"))\n",
    "    #「語」表記は5倍して返す。ただし、そのうち分野区分マスタに英文か和文かを示すダミー変数を作成して試したいので、ここは後で直す予定。\n",
    "    for i in y:\n",
    "        if i in str(df['文字数']):\n",
    "            return int(df['文字数'].replace(i,\"\"))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "#定義したルールをapplyでdfの全てに対して一行ずつ適応している。applyはよく使う。\n",
    "df['置換後の文字数']=df.apply(make_mojisu,axis = 1)\n",
    "\n",
    "\"\"\"解答言語が英語の場合、文字数を5倍にする\"\"\"\n",
    "#5倍するラムダ関数\n",
    "def times_five(df):\n",
    "    if df[\"解答言語\"]== \"英語\":\n",
    "        return df[\"置換後の文字数\"]*5\n",
    "    else:\n",
    "        return df[\"置換後の文字数\"]\n",
    "#定義したルールをapplyでdfの全てに対して一行ずつ適応している。applyはよく使う。\n",
    "df['置換後の文字数5']=df.apply(times_five,axis = 1)\n",
    "\n",
    "\"\"\"ポイント数の修正\"\"\"\n",
    "\n",
    "name_col = \"ポイント数\"\n",
    "lst = []\n",
    "df[name_col] = df[name_col].replace('\\u3000',np.nan)\n",
    "df[name_col] = df[name_col].replace('',np.nan)\n",
    "df[name_col] = df[name_col].replace('2+7+8(ポ5)',\"2+7+8\")\n",
    "df[name_col] = df[name_col].replace('2+4+6（ポ3）',\"2+4+6\")\n",
    "                             \n",
    "for i in df[name_col]:\n",
    "    if type(i) == str:\n",
    "        x = i.split('+')\n",
    "        y = list(map(int, x))\n",
    "        i = sum(y)\n",
    "    lst.append(i)\n",
    "\n",
    "df['置換後のポイント数'] = lst\n",
    "\n",
    "\"\"\"配点の修正\"\"\"\n",
    "# df = raw_df\n",
    "name_col = \"配点\"\n",
    "lst = []\n",
    "df[name_col] = df[name_col].replace('\\u3000',np.nan)\n",
    "df[name_col] = df[name_col].replace('',np.nan)\n",
    "df[name_col] = df[name_col].replace('5(2+2+1)+5',10)\n",
    "df[name_col] = df[name_col].replace('2*2',4)\n",
    "                             \n",
    "for i in df[name_col]:\n",
    "    if type(i) == str:\n",
    "        if '+' in i:\n",
    "            x = i.split('+')\n",
    "        elif '＋' in i:\n",
    "            x = i.split('＋')\n",
    "        y = list(map(int, x))\n",
    "        i = sum(y)\n",
    "    lst.append(i)\n",
    "\n",
    "df['置換後の配点'] = lst\n",
    "\n",
    "df_1b_cleaned = df #読み込んだデータに分野を足してデータクレンジングしたもの\n",
    "\n",
    "\"\"\"\n",
    "欠損値を分野別、科目別平均値で補完\n",
    "1.　pandasでDFの形でデータを読み込む\n",
    "2.　df.queryで分野2を条件に、カテゴリごとのDFに分ける\n",
    "3.　分けたDFで、平均値で欠損値を埋める\n",
    "4.　df.appendで分けたDFを結合\n",
    "\"\"\"\n",
    "df = df_1b_cleaned.copy()\n",
    "list_bunya = df['分野名_修正v2'].unique()\n",
    "list_fillna = ['置換後の文字数','置換後の文字数5','置換後のポイント数','置換後の配点']\n",
    "\n",
    "df_ret=[]\n",
    "for i in list_bunya:\n",
    "    ret = df[df['分野名_修正v2'] == i]\n",
    "    for j in list_fillna:\n",
    "        ret[j] =ret[j].fillna(ret[j].mean())\n",
    "    df_ret.append(ret)\n",
    "\n",
    "dfs = df_ret[0] #集計結果を一つのDFに結合\n",
    "for i in df_ret[1:]:\n",
    "    dfs = dfs.append(i)\n",
    "\n",
    "df_1b_cleaned_filled = dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ba70bc70-0a08-4cfe-b0e0-75aaac4d9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Excelの書き出し \n",
    "\"\"\"\n",
    "\n",
    "df = df_1b_cleaned\n",
    "\n",
    "name_excel_output = \"crlea_bunya_dm_0613received_0613cleaned_test.csv\"\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/intermediatedata\"#フォルダパス\n",
    "\n",
    "df.to_csv('{}/{}'.format(path_folder,name_excel_output),encoding='utf-8-sig',index=False)\n",
    "\n",
    "name_excel_output = \"crlea_bunya_dm_0613received_0613cleaned_test.xlsx\"\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/intermediatedata\"#Excelが置いてあるフォルダパス\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('{}/{}'.format(path_folder,name_excel_output)) as writer:\n",
    "    df.to_excel(writer, sheet_name='data1',encoding='utf-8-sig', index = False)\n",
    "    \n",
    "df = df_1b_cleaned_filled\n",
    "\n",
    "name_excel_output = \"crlea_bunya_dm_0613received_0613cleaned_filled_test.csv\"\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/intermediatedata\"#フォルダパス\n",
    "\n",
    "df.to_csv('{}/{}'.format(path_folder,name_excel_output),encoding='utf-8-sig',index=False)\n",
    "\n",
    "name_excel_output = \"crlea_bunya_dm_0613received_0613cleaned_filled_test.xlsx\"\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/intermediatedata\"#Excelが置いてあるフォルダパス\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('{}/{}'.format(path_folder,name_excel_output)) as writer:\n",
    "    df.to_excel(writer, sheet_name='data1',encoding='utf-8-sig', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f8f5e-c319-4e63-8814-18c1325b5d80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 機械学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e3014-4f8c-4398-b488-86f4ccdbe7c8",
   "metadata": {},
   "source": [
    "#データ読み込み マークダウンにしてあるから必要ならコードセルにする\n",
    "\"\"\"\n",
    "data1を加工したcsvファイルを読み込む場合　\n",
    "\"\"\"\n",
    "\n",
    "name_csv = \"crlea_bunya_dm_0613received_0613cleaned_test.csv\" #ファイル名\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/intermediatedata\"#データが置いてあるフォルダパス\n",
    "path_file = r'{p}/{n}'.format(p = path_folder, n = name_csv)#ファイルパスとファイル名\n",
    "\n",
    "# csvファイルの読み込みと空のリストに追加\n",
    "df = pd.read_csv(filepath_or_buffer = path_file, sep=\",\",\n",
    "                 usecols=['科目',\n",
    "                          '分野名_修正v1',\n",
    "                          '分野名_修正v2',\n",
    "                          'ポイント採点',\n",
    "                          '年度',\n",
    "                          '採点回',\n",
    "                          '学年',\n",
    "                          '置換後のポイント数',\n",
    "                          '置換後の文字数',\n",
    "                          '置換後の文字数5',\n",
    "                          '解答言語',\n",
    "                          '置換後の配点',\n",
    "                          '企画ペース',\n",
    "                          '最終ペース'])\n",
    "#列のリネーム\n",
    "df = df.rename(columns={'分野名_修正v1':'分野1',\n",
    "                        '分野名_修正v2':'分野2',\n",
    "                        '置換後のポイント数':'ポイント数',\n",
    "                        '置換後の文字数':'文字数',#英語1単語も1文字として数えた\n",
    "                        '置換後の文字数5':'文字数5',#英語１単語を5文字とした。前の分析ではこっち。精度を比較する。\n",
    "                        '置換後の配点':'配点'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5c91fbf6-d45e-4246-9cf9-470f834c2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"データ選択\"\"\"\n",
    "\n",
    "def select_data(df):\n",
    "    df = df[['科目',\n",
    "        '分野名_修正v1',\n",
    "        '分野名_修正v2',\n",
    "        'ポイント採点',\n",
    "        '年度',\n",
    "        '採点回',\n",
    "        '学年',\n",
    "        '置換後のポイント数',\n",
    "        '置換後の文字数',\n",
    "        '置換後の文字数5',\n",
    "         '解答言語',\n",
    "        '置換後の配点',\n",
    "        '企画ペース',\n",
    "        '最終ペース']]\n",
    "    df = df.rename(columns={'分野名_修正v1':'分野1',\n",
    "                        '分野名_修正v2':'分野2',\n",
    "                        '置換後のポイント数':'ポイント数',\n",
    "                        '置換後の文字数':'文字数',#英語1単語も1文字として数えた\n",
    "                        '置換後の文字数5':'文字数5',#英語１単語を5文字とした。前の分析ではこっち。精度を比較する。\n",
    "                        '置換後の配点':'配点'})\n",
    "    return df\n",
    "\n",
    "df_ml = select_data(df_1b_cleaned)\n",
    "df_ml_filled = select_data(df_1b_cleaned_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e1cd74-2b99-44c8-8bab-6d91faa9653c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DecisionTreeRegressorメソッドの概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9115482-7b20-4f68-8ea8-45b367e2f4e4",
   "metadata": {},
   "source": [
    "|引数名|概要|デフォルト|\n",
    "| :---- | :---- | :---- |\n",
    "|criterion|不純度を測定する基準（平均二乗誤差、平均絶対誤差など）|‘mse’|\n",
    "|splitter|条件探索アルゴリズムを選択するオプション（’best’と’rondom’が指定可能）|‘best’|\n",
    "|max_depth|決定木のノード深さの制限値。ツリーが深くなりすぎて過学習の状態に陥った際は、このパラメータが正則化の役割を果たす。|None|\n",
    "|min_samples_split|ノードを分割するために必要なサンプルの最小値|2|\n",
    "|min_samples_leaf|1ノードの深さを作成するために必要となるデータ数の最小値。指定した値以上のデータ数を持たないノードは作られない。|1|\n",
    "|min_weight_fraction_leaf|サンプルの重みを考慮した上でのmin_samples_leafに該当|0.0|\n",
    "|max_features|ランダムに指定する説明変数の数(全ての説明変数がモデル学習に活用されるわけではなく、ランダムに割り振られる）|None|\n",
    "|random_state|乱数シード|None|\n",
    "|max_leaf_nodes|作成される決定木の葉の数を、指定した値以下に制御する|None|\n",
    "|min_impurity_decrease|決定木の成長の早期停止するための閾値。不純度が指定の値より減少した場合、ノードを分岐し、不純度が指定の値より減少しなければ分岐を抑制。|0.0|\n",
    "|ccp_alpha|ccp_alphaが大きいほどプルーニングされるノードの数が増加。プルーニングとは、精度低下をできるだけ抑えながら過剰な重みを排除するプロセスを指す。|0.0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b81a0-3fe3-4721-815b-04fd8f09c98b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d8f678ab-13d6-4106-b6b0-f63da9b3ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1単語を文字数5で数える\n",
    "機械学習用データを準備する関数\n",
    "df[0]:21夏1年生モデル：検証データ ＝ 21年夏1学年\n",
    "df[1]:21夏2年生モデル：検証データ ＝ 21年夏2学年\n",
    "df[2]:21夏3年生モデル：検証データ ＝ 21年夏3学年\n",
    "df[3]:21夏学年混合モデル：検証データ ＝ 21年夏全学年\n",
    "\"\"\"\n",
    "\n",
    "def make_mldata(df,str):\n",
    "    \"\"\"\n",
    "    df:使用するデータ\n",
    "    str:抽出する科目\n",
    "    1.選択した科目を抽出\n",
    "    2.one hot encoding\n",
    "    3.説明変数、目的変数でデータフレームを分離\n",
    "    4.訓練用、検証用にデータフレームを分離\n",
    "        1.21夏1年生モデル：検証データ ＝ 21年夏1学年\n",
    "        2.21夏2年生モデル：検証データ ＝ 21年夏2学年\n",
    "        3.21夏3年生モデル：検証データ ＝ 21年夏3学年\n",
    "        4.21夏学年混合モデル：検証データ ＝ 21年夏全学年\n",
    "    5.説明変数から採点回を削除 \n",
    "    \"\"\"\n",
    "    #0.NAN remove 追加した\n",
    "    df = df.dropna(subset=['ポイント数', '配点', '文字数'])\n",
    "    df.loc[:,'ポイント数'] =df.loc[:,'ポイント数'].round(0).astype(int)\n",
    "    df.loc[:,'配点'] = df.loc[:,'配点'].round(0).astype(int)\n",
    "    df.loc[:,'文字数'] = df.loc[:,'文字数'].round(0).astype(int)\n",
    "    df.loc[:,'文字数5'] = df.loc[:,'文字数5'].round(0).astype(int)\n",
    "    #1.科目を抽出し、科目と分野1の列を削除\n",
    "    df = df[df['科目']== str].drop(columns = [\"科目\",\"分野1\",\"文字数\",\"解答言語\"])#解答言語を考慮しない\n",
    "    \n",
    "    #2.名義変数のエンコーディング pandas get_dummies関数でone hot encording\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    #3.上記のデータセットを説明変数と目的変数で分ける\n",
    "    df_X = df.drop(columns = \"最終ペース\")#目的変数を除外した（説明変数だけ含む）データフレーム\n",
    "    df_y = df.loc[:,['年度','採点回_夏','学年','最終ペース']]#目的変数と選択用変数だけ含むデータフレーム\n",
    "    \n",
    "    #4.データフレームの分離 不恰好なのをどうにかしたい\n",
    "    \"\"\"秋冬が訓練用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_train_1 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==1))]\n",
    "    X_train_2 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==2))]\n",
    "    X_train_3 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==3))]\n",
    "    X_train_4 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21))]\n",
    "    #訓練用目的変数\n",
    "    y_train_1 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==1))]\n",
    "    y_train_2 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==2))]\n",
    "    y_train_3 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==3))]\n",
    "    y_train_4 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21))]\n",
    "\n",
    "    \"\"\"夏が検証用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_test_1 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==1)]\n",
    "    X_test_2 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==2)]\n",
    "    X_test_3 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==3)]\n",
    "    X_test_4 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)]\n",
    "    #訓練用目的変数\n",
    "    y_test_1 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==1)]\n",
    "    y_test_2 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==2)]\n",
    "    y_test_3 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==3)]\n",
    "    y_test_4 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)]\n",
    "   \n",
    "    #5.不要な変数を削除\n",
    "    rem_cols_x = [\"企画ペース\",\"採点回_夏\",\"採点回_秋\",\"採点回_冬\",\"年度\"]\n",
    "    rem_cols_xb = [\"採点回_夏\",\"採点回_秋\",\"採点回_冬\",\"年度\"]\n",
    "    rem_cols_y = [\"採点回_夏\",\"年度\",\"学年\"]\n",
    "\n",
    "    #説明変数\n",
    "    X_train_1 = X_train_1.drop(columns = rem_cols_x)\n",
    "    X_train_2 = X_train_2.drop(columns = rem_cols_x)\n",
    "    X_train_3 = X_train_3.drop(columns = rem_cols_x)\n",
    "    X_train_4 = X_train_4.drop(columns = rem_cols_x)\n",
    "\n",
    "    #出力用に企画ペースを残している\n",
    "    X_test_1b = X_test_1.drop(columns = rem_cols_xb)\n",
    "    X_test_2b = X_test_2.drop(columns = rem_cols_xb)\n",
    "    X_test_3b = X_test_3.drop(columns = rem_cols_xb)\n",
    "    X_test_4b = X_test_4.drop(columns = rem_cols_xb)    \n",
    "    \n",
    "    X_test_1 = X_test_1.drop(columns = rem_cols_x)\n",
    "    X_test_2 = X_test_2.drop(columns = rem_cols_x)\n",
    "    X_test_3 = X_test_3.drop(columns = rem_cols_x)\n",
    "    X_test_4 = X_test_4.drop(columns = rem_cols_x)\n",
    "         \n",
    "    #目的変数\n",
    "    y_train_1 = y_train_1.drop(columns = rem_cols_y)\n",
    "    y_train_2 = y_train_2.drop(columns = rem_cols_y)\n",
    "    y_train_3 = y_train_3.drop(columns = rem_cols_y)\n",
    "    y_train_4 = y_train_4.drop(columns = rem_cols_y)\n",
    "\n",
    "    y_test_1 = y_test_1.drop(columns = rem_cols_y)\n",
    "    y_test_2 = y_test_2.drop(columns = rem_cols_y)\n",
    "    y_test_3 = y_test_3.drop(columns = rem_cols_y)\n",
    "    y_test_4 = y_test_4.drop(columns = rem_cols_y)\n",
    "    \n",
    "    #各モデル用データを各データフレームにまとめ、それらをさらにデータフレームにまとめて返す。\n",
    "    df_res=[]\n",
    "    data1=[X_train_1,y_train_1,X_test_1,y_test_1,X_test_1b]#1年生モデルデータ なぜかdata1はリストとして入っていて、X_trainとかはdf。\n",
    "    data2=[X_train_2,y_train_2,X_test_2,y_test_2,X_test_2b]#2年生モデルデータ\n",
    "    data3=[X_train_3,y_train_3,X_test_3,y_test_3,X_test_3b]#3年生モデルデータ\n",
    "    data4=[X_train_4,y_train_4,X_test_4,y_test_4,X_test_4b]#学年混合モデルデータ\n",
    "    df_res = [data1, data2, data3, data4]\n",
    "\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "131e674b-b138-45f0-a02c-17082bfd392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1単語を文字数5で数える。数学の分野を大まかに区分。\n",
    "機械学習用データを準備する関数\n",
    "df[0]:21夏1年生モデル：検証データ ＝ 21年夏1学年\n",
    "df[1]:21夏2年生モデル：検証データ ＝ 21年夏2学年\n",
    "df[2]:21夏3年生モデル：検証データ ＝ 21年夏3学年\n",
    "df[3]:21夏学年混合モデル：検証データ ＝ 21年夏全学年\n",
    "\"\"\"\n",
    "\n",
    "def make_mldata_all(df,str):\n",
    "    \"\"\"\n",
    "    df:使用するデータ\n",
    "    str:抽出する科目\n",
    "    1.選択した科目を抽出\n",
    "    2.one hot encoding\n",
    "    3.説明変数、目的変数でデータフレームを分離\n",
    "    4.訓練用、検証用にデータフレームを分離\n",
    "        1.21夏1年生モデル：検証データ ＝ 21年夏1学年\n",
    "        2.21夏2年生モデル：検証データ ＝ 21年夏2学年\n",
    "        3.21夏3年生モデル：検証データ ＝ 21年夏3学年\n",
    "        4.21夏学年混合モデル：検証データ ＝ 21年夏全学年\n",
    "    5.説明変数から採点回を削除 \n",
    "    \"\"\"\n",
    "    #0.NAN remove 追加した\n",
    "    df = df.dropna(subset=['ポイント数', '配点', '文字数'])\n",
    "    df.loc[:,'ポイント数'] =df.loc[:,'ポイント数'].round(0).astype(int)\n",
    "    df.loc[:,'配点'] = df.loc[:,'配点'].round(0).astype(int)\n",
    "    df.loc[:,'文字数'] = df.loc[:,'文字数'].round(0).astype(int)\n",
    "    df.loc[:,'文字数5'] = df.loc[:,'文字数5'].round(0).astype(int)\n",
    "    #1.科目を抽出し、科目と分野1の列を削除\n",
    "    df = df.drop(columns = [\"分野1\",\"文字数\",\"解答言語\"])#解答言語を考慮しない\n",
    "    \n",
    "    #2.名義変数のエンコーディング pandas get_dummies関数でone hot encording\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    #3.上記のデータセットを説明変数と目的変数で分ける\n",
    "    df_X = df.drop(columns = \"最終ペース\")#目的変数を除外した（説明変数だけ含む）データフレーム\n",
    "    df_y = df.loc[:,['年度','採点回_夏','学年','最終ペース']]#目的変数と選択用変数だけ含むデータフレーム\n",
    "    \n",
    "    #4.データフレームの分離 不恰好なのをどうにかしたい\n",
    "    \"\"\"秋冬が訓練用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_train_1 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==1))]\n",
    "    X_train_2 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==2))]\n",
    "    X_train_3 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==3))]\n",
    "    X_train_4 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21))]\n",
    "    #訓練用目的変数\n",
    "    y_train_1 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==1))]\n",
    "    y_train_2 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==2))]\n",
    "    y_train_3 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==3))]\n",
    "    y_train_4 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21))]\n",
    "\n",
    "    \"\"\"夏が検証用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_test_1 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==1)]\n",
    "    X_test_2 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==2)]\n",
    "    X_test_3 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==3)]\n",
    "    X_test_4 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)]\n",
    "    #訓練用目的変数\n",
    "    y_test_1 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==1)]\n",
    "    y_test_2 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==2)]\n",
    "    y_test_3 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==3)]\n",
    "    y_test_4 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)]\n",
    "   \n",
    "    #5.不要な変数を削除\n",
    "    rem_cols_x = [\"企画ペース\",\"採点回_夏\",\"採点回_秋\",\"採点回_冬\",\"年度\"]\n",
    "    rem_cols_xb = [\"採点回_夏\",\"採点回_秋\",\"採点回_冬\",\"年度\"]\n",
    "    rem_cols_y = [\"採点回_夏\",\"年度\",\"学年\"]\n",
    "\n",
    "    #説明変数\n",
    "    X_train_1 = X_train_1.drop(columns = rem_cols_x)\n",
    "    X_train_2 = X_train_2.drop(columns = rem_cols_x)\n",
    "    X_train_3 = X_train_3.drop(columns = rem_cols_x)\n",
    "    X_train_4 = X_train_4.drop(columns = rem_cols_x)\n",
    "\n",
    "    #出力用に企画ペースを残している\n",
    "    X_test_1b = X_test_1.drop(columns = rem_cols_xb)\n",
    "    X_test_2b = X_test_2.drop(columns = rem_cols_xb)\n",
    "    X_test_3b = X_test_3.drop(columns = rem_cols_xb)\n",
    "    X_test_4b = X_test_4.drop(columns = rem_cols_xb)    \n",
    "    \n",
    "    X_test_1 = X_test_1.drop(columns = rem_cols_x)\n",
    "    X_test_2 = X_test_2.drop(columns = rem_cols_x)\n",
    "    X_test_3 = X_test_3.drop(columns = rem_cols_x)\n",
    "    X_test_4 = X_test_4.drop(columns = rem_cols_x)\n",
    "         \n",
    "    #目的変数\n",
    "    y_train_1 = y_train_1.drop(columns = rem_cols_y)\n",
    "    y_train_2 = y_train_2.drop(columns = rem_cols_y)\n",
    "    y_train_3 = y_train_3.drop(columns = rem_cols_y)\n",
    "    y_train_4 = y_train_4.drop(columns = rem_cols_y)\n",
    "\n",
    "    y_test_1 = y_test_1.drop(columns = rem_cols_y)\n",
    "    y_test_2 = y_test_2.drop(columns = rem_cols_y)\n",
    "    y_test_3 = y_test_3.drop(columns = rem_cols_y)\n",
    "    y_test_4 = y_test_4.drop(columns = rem_cols_y)\n",
    "    \n",
    "    #各モデル用データを各データフレームにまとめ、それらをさらにデータフレームにまとめて返す。\n",
    "    df_res=[]\n",
    "    data1=[X_train_1,y_train_1,X_test_1,y_test_1,X_test_1b]#1年生モデルデータ なぜかdata1はリストとして入っていて、X_trainとかはdf。\n",
    "    data2=[X_train_2,y_train_2,X_test_2,y_test_2,X_test_2b]#2年生モデルデータ\n",
    "    data3=[X_train_3,y_train_3,X_test_3,y_test_3,X_test_3b]#3年生モデルデータ\n",
    "    data4=[X_train_4,y_train_4,X_test_4,y_test_4,X_test_4b]#学年混合モデルデータ\n",
    "    df_res = [data1, data2, data3, data4]\n",
    "\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e196106a-d8da-48d5-a57c-080366e33e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "解答言語と文字数,分野2\n",
    "機械学習用データを準備する関数\n",
    "df[0]:21夏1年生モデル：検証データ ＝ 21年夏1学年\n",
    "df[1]:21夏2年生モデル：検証データ ＝ 21年夏2学年\n",
    "df[2]:21夏3年生モデル：検証データ ＝ 21年夏3学年\n",
    "df[3]:21夏学年混合モデル：検証データ ＝ 21年夏全学年\n",
    "\"\"\"\n",
    "\n",
    "def make_mldata2(df,str):\n",
    "    \"\"\"\n",
    "    df:使用するデータ\n",
    "    str:抽出する科目\n",
    "    1.選択した科目を抽出\n",
    "    2.one hot encoding\n",
    "    3.説明変数、目的変数でデータフレームを分離\n",
    "    4.訓練用、検証用にデータフレームを分離\n",
    "        1.21夏1年生モデル：検証データ ＝ 21年夏1学年\n",
    "        2.21夏2年生モデル：検証データ ＝ 21年夏2学年\n",
    "        3.21夏3年生モデル：検証データ ＝ 21年夏3学年\n",
    "        4.21夏学年混合モデル：検証データ ＝ 21年夏全学年\n",
    "    5.説明変数から採点回を削除 \n",
    "    \"\"\"\n",
    "    #0.NAN remove 追加した\n",
    "    df = df.dropna(subset=['ポイント数', '配点', '文字数'])\n",
    "    df.loc[:,'ポイント数'] =df.loc[:,'ポイント数'].round(0).astype(int)\n",
    "    df.loc[:,'配点'] = df.loc[:,'配点'].round(0).astype(int)\n",
    "    df.loc[:,'文字数'] = df.loc[:,'文字数'].round(0).astype(int)\n",
    "    df.loc[:,'文字数5'] = df.loc[:,'文字数5'].round(0).astype(int)\n",
    "    #1.科目を抽出し、科目と分野1の列を削除\n",
    "    df = df[df['科目']== str].drop(columns = [\"科目\",\"分野1\",\"文字数5\"])#解答言語を考慮しない\n",
    "    \n",
    "    #2.名義変数のエンコーディング pandas get_dummies関数でone hot encording\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    #3.上記のデータセットを説明変数と目的変数で分ける\n",
    "    df_X = df.drop(columns = \"最終ペース\")#目的変数を除外した（説明変数だけ含む）データフレーム\n",
    "    df_y = df.loc[:,['年度','採点回_夏','学年','最終ペース']]#目的変数と選択用変数だけ含むデータフレーム\n",
    "    \n",
    "    #4.データフレームの分離 不恰好なのをどうにかしたい\n",
    "    \"\"\"秋冬が訓練用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_train_1 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==1))]\n",
    "    X_train_2 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==2))]\n",
    "    X_train_3 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==3))]\n",
    "    X_train_4 = df_X[~((df_X['採点回_夏'] == 1)&(df_X['年度']==21))]\n",
    "    #訓練用目的変数\n",
    "    y_train_1 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==1))]\n",
    "    y_train_2 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==2))]\n",
    "    y_train_3 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==3))]\n",
    "    y_train_4 = df_y[~((df_y['採点回_夏'] == 1)&(df_y['年度']==21))]\n",
    "\n",
    "    \"\"\"夏が検証用\"\"\"\n",
    "    #訓練用説明変数\n",
    "    X_test_1 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==1)]\n",
    "    X_test_2 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==2)]\n",
    "    X_test_3 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)&(df_X['学年']==3)]\n",
    "    X_test_4 = df_X[(df_X['採点回_夏'] == 1)&(df_X['年度']==21)]\n",
    "    #訓練用目的変数\n",
    "    y_test_1 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==1)]\n",
    "    y_test_2 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==2)]\n",
    "    y_test_3 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)&(df_y['学年']==3)]\n",
    "    y_test_4 = df_y[(df_y['採点回_夏'] == 1)&(df_y['年度']==21)]\n",
    "   \n",
    "    #5.不要な変数を削除\n",
    "    rem_cols_x = [\"企画ペース\",\"採点回_夏\",\"採点回_秋\",\"採点回_冬\",\"年度\"]\n",
    "    rem_cols_xb = [\"採点回_夏\",\"採点回_秋\",\"採点回_冬\",\"年度\"]\n",
    "    rem_cols_y = [\"採点回_夏\",\"年度\",\"学年\"]\n",
    "\n",
    "    #説明変数\n",
    "    X_train_1 = X_train_1.drop(columns = rem_cols_x)\n",
    "    X_train_2 = X_train_2.drop(columns = rem_cols_x)\n",
    "    X_train_3 = X_train_3.drop(columns = rem_cols_x)\n",
    "    X_train_4 = X_train_4.drop(columns = rem_cols_x)\n",
    "\n",
    "    #出力用に企画ペースを残している\n",
    "    X_test_1b = X_test_1.drop(columns = rem_cols_xb)\n",
    "    X_test_2b = X_test_2.drop(columns = rem_cols_xb)\n",
    "    X_test_3b = X_test_3.drop(columns = rem_cols_xb)\n",
    "    X_test_4b = X_test_4.drop(columns = rem_cols_xb)    \n",
    "    \n",
    "    X_test_1 = X_test_1.drop(columns = rem_cols_x)\n",
    "    X_test_2 = X_test_2.drop(columns = rem_cols_x)\n",
    "    X_test_3 = X_test_3.drop(columns = rem_cols_x)\n",
    "    X_test_4 = X_test_4.drop(columns = rem_cols_x)\n",
    "         \n",
    "    #目的変数\n",
    "    y_train_1 = y_train_1.drop(columns = rem_cols_y)\n",
    "    y_train_2 = y_train_2.drop(columns = rem_cols_y)\n",
    "    y_train_3 = y_train_3.drop(columns = rem_cols_y)\n",
    "    y_train_4 = y_train_4.drop(columns = rem_cols_y)\n",
    "\n",
    "    y_test_1 = y_test_1.drop(columns = rem_cols_y)\n",
    "    y_test_2 = y_test_2.drop(columns = rem_cols_y)\n",
    "    y_test_3 = y_test_3.drop(columns = rem_cols_y)\n",
    "    y_test_4 = y_test_4.drop(columns = rem_cols_y)\n",
    "    \n",
    "    #各モデル用データを各データフレームにまとめ、それらをさらにデータフレームにまとめて返す。\n",
    "    df_res=[]\n",
    "    data1=[X_train_1,y_train_1,X_test_1,y_test_1,X_test_1b]#1年生モデルデータ なぜかdata1はリストとして入っていて、X_trainとかはdf。\n",
    "    data2=[X_train_2,y_train_2,X_test_2,y_test_2,X_test_2b]#2年生モデルデータ\n",
    "    data3=[X_train_3,y_train_3,X_test_3,y_test_3,X_test_3b]#3年生モデルデータ\n",
    "    data4=[X_train_4,y_train_4,X_test_4,y_test_4,X_test_4b]#学年混合モデルデータ\n",
    "    df_res = [data1, data2, data3, data4]\n",
    "\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce4f56e3-afa1-4494-947c-01e779ddfa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://stackoverflow.com/questions/50607740/reverse-a-get-dummies-encoding-in-pandas\n",
    "ダミー変数を元に戻す。エクセル出力用\n",
    "\"\"\"\n",
    "\n",
    "def undummify(df, prefix_sep=\"_\"):\n",
    "    cols2collapse = {\n",
    "        item.split(prefix_sep)[0]: (prefix_sep in item) for item in df.columns\n",
    "    }\n",
    "    series_list = []\n",
    "    for col, needs_to_collapse in cols2collapse.items():\n",
    "        if needs_to_collapse:\n",
    "            undummified = (\n",
    "                df.filter(like=col)\n",
    "                .idxmax(axis=1)\n",
    "                .apply(lambda x: x.split(prefix_sep, maxsplit=1)[1])\n",
    "                .rename(col)\n",
    "            )\n",
    "            series_list.append(undummified)\n",
    "        else:\n",
    "            series_list.append(df[col])\n",
    "    undummified_df = pd.concat(series_list, axis=1)\n",
    "    return undummified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "954f8a63-9407-4acc-b571-f0873bc17f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make_mldataのseriesを使って、予測値を出し、予測値、最終ペース、差分、乖離度（予測値/最終ペース）を列に追加したdfを返す。\n",
    "\"\"\"\n",
    "def test_model2(df,a=2,b=1,c=None):\n",
    "    X_train= df[0]\n",
    "    y_train= df[1]\n",
    "    X_test = df[2]\n",
    "    y_test = df[3]\n",
    "    X_test_b = df[4]\n",
    "    model = DecisionTreeRegressor(criterion='mse', \n",
    "                                   splitter='best', \n",
    "                                   max_depth=c, \n",
    "                                   min_samples_split=a, #3,4,5とか？\n",
    "                                   min_samples_leaf=b,#2とか \n",
    "                                   min_weight_fraction_leaf=0.0,\n",
    "                                   max_features=None, \n",
    "                                   random_state=None, \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   min_impurity_decrease=0.0, \n",
    "                                   ccp_alpha=0.0\n",
    "                                  )\n",
    "\n",
    "    #上記のパラメータでモデルを学習する\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred  = model.predict(X_test)\n",
    "    #得た結果の樹形図を表示する\n",
    "    \n",
    "    #得た結果を学習データとマージしてデータフレームで返す\n",
    "    df_res=[]\n",
    "    \n",
    "    #検証に使った説明変数データのダミー変数を元に戻す\n",
    "    df_res = undummify(X_test_b)  #企画ペースを入れるためにここをいじった。元はX_test\n",
    "    df_res.loc[:,'AI想定ペース']= y_pred #上のデータに予測値をマージ\n",
    "    df_res.loc[:,'最終ペース']= y_test\n",
    "    df_res.loc[:,'誤差']= df_res['AI想定ペース']-df_res['最終ペース']#実測値と予測値の差分を列に追加 \n",
    "    df_res.loc[:,'AI乖離度']= df_res['AI想定ペース']/df_res['最終ペース']#実測値と予測値の差分を列に追加 \n",
    "    df_res.loc[:,'元の乖離度']= df_res['企画ペース']/df_res['最終ペース']#実測値と予測値の差分を列に追加 \n",
    "            \n",
    "    # 最終ペースをランク分け\n",
    "    x = \"最終ペース\"\n",
    "    conditions = [\n",
    "        (df_res[x] >= 600),\n",
    "        (df_res[x] >= 500),\n",
    "        (df_res[x] >= 400),\n",
    "        (df_res[x] >= 300),\n",
    "        (df_res[x] >= 200),\n",
    "        (df_res[x] >= 100),\n",
    "        (df_res[x] >= 0)\n",
    "         ]\n",
    "\n",
    "    choices = [\"600~\", \"500~600\", \"400~500\", \"300~400\", \"200~300\", \"100~200\",\"0~100\"]\n",
    "\n",
    "    df_res.loc[:,'最終ランク'] = np.select(conditions, choices, default = 0)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6babecb-2620-4619-a4c5-536e46e749e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用ライブラリ\n",
    "# from sklearn import tree\n",
    "# import graphviz\n",
    "\"\"\"\n",
    "make_mldataのデータを使って樹形図を作成し、pdfに保存する。\n",
    "\n",
    "df_input:入力データ\n",
    "a:作成するファイルの3つめの単語\n",
    "b:作成するファイルの4つめの単語\n",
    "1. 科目のリストを作成\n",
    "2. モデルのリストを作成\n",
    "3. 科目、モデルの順で樹形図を作成し、保存\n",
    "\"\"\"\n",
    "def make_leaf3(df_input,a=2,b=1,c=None):\n",
    "    kamoku_name=[\"国語\",\"英語\",\"数学\"]\n",
    "    model_name=[\"1年\",\"2年\",\"3年\",\"学年混合\"]\n",
    "    model = DecisionTreeRegressor(criterion='mse', \n",
    "                                   splitter='best', \n",
    "                                   max_depth=c, \n",
    "                                   min_samples_split=a, #3,4,5とか？\n",
    "                                   min_samples_leaf=b,#2とか \n",
    "                                   min_weight_fraction_leaf=0.0,\n",
    "                                   max_features=None, \n",
    "                                   random_state=None, \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   min_impurity_decrease=0.0, \n",
    "                                   ccp_alpha=0.0\n",
    "                                  )\n",
    "    df = []\n",
    "    for i in kamoku_name:\n",
    "        df = make_mldata(df_input,i)\n",
    "        for j in range(0,len(model_name)):\n",
    "            X_train= df[j][0]\n",
    "            y_train= df[j][1]\n",
    "            X_test = df[j][2]\n",
    "            y_test = df[j][3]\n",
    "            X_train_feature_names = X_train.columns.values.tolist() \n",
    "            \n",
    "            #上記のパラメータでモデルを学習する\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            dot_data = tree.export_graphviz(model, out_file=None,\n",
    "                                            feature_names= X_train_feature_names,\n",
    "                                            # class_names=iris.target_names,\n",
    "                                            filled=True, rounded=True, special_characters=True\n",
    "                                           )\n",
    "            graph = graphviz.Source(dot_data) \n",
    "            \"\"\"グラフの保存\"\"\"\n",
    "            name_file = \"DTs_{}_{}_mss{}_msl{}_md{}\".format(i,model_name[j],str(a),str(b),str(c)) #ファイル名\n",
    "            path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/output\"#フォルダパス\n",
    "            path_file = r'{p}/{n}'.format(p = path_folder, n = name_file)#ファイルパスとファイル名\n",
    "            graph.render(path_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04731d-161c-47f6-93b4-48e3fe78f334",
   "metadata": {},
   "source": [
    "# 結果出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "74ac379b-f639-4c34-9c7a-9ca4b7cc491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"欠損値を補完したデータの予測値\"\"\"\n",
    "#予測値と統計値を各科目のモデルごとにエクセルシートに出力\n",
    "\n",
    "\n",
    "kamoku=[\"国語\",\"英語\",\"数学\"]#,\"数学\"は文字数がないため、dropnaで除外してしまっている。前処理で変更する必要あり。\n",
    "model=[\"1年\",\"2年\",\"3年\",\"学年混合\"]\n",
    "\n",
    "mss= 2#min sample split\n",
    "msl= 1#min sample leaf\n",
    "md=None #max depth\n",
    "test_num=\"June3\"\n",
    "\n",
    "name_file = \"DTs_{}_mss{}_msl{}_md{}.xlsx\".format(str(test_num),str(mss),str(msl),str(md)) #ファイル名\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/output\"#フォルダパス\n",
    "# path_file = r'{p}/{n}'.format(p = path_folder, n = name_file)#ファイルパスとファイル名\n",
    "\n",
    "# 入力データ\n",
    "df = df_ml_filled\n",
    "df1=[[[],[],[],[]],[[],[],[],[]],[[],[],[],[]]]\n",
    "df2=[[[],[],[],[]],[[],[],[],[]],[[],[],[],[]]]\n",
    "for i,n in enumerate(kamoku):\n",
    "    for j,m in enumerate(model):\n",
    "        df1[i][j] = test_model2(make_mldata(df,kamoku[i])[j],mss,msl,md)\n",
    "        df2[i][j] = pandas_tool.summary(df1[i][j].loc[:,['最終ペース','企画ペース','AI想定ペース','誤差','AI乖離度','元の乖離度']])[0]\n",
    "        \n",
    "with pd.ExcelWriter('{}/{}'.format(path_folder,name_file)) as writer:\n",
    "    for i,n in enumerate(kamoku):\n",
    "        for j,m in enumerate(model):\n",
    "            s_names=['{}_{}_values'.format(kamoku[i],model[j]),'{}_{}_stats'.format(kamoku[i],model[j])]\n",
    "            df1[i][j].to_excel(writer, sheet_name='{}'.format(s_names[0]))\n",
    "            df2[i][j].to_excel(writer, sheet_name='{}'.format(s_names[1]))\n",
    "            \n",
    "            \n",
    "# =IFS(M2>=600,6,M2>=500,5,M2>=400,4,M2>=300,3,M2>=200,2,M2>=100,1,TRUE,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "db2582f4-19fb-470c-acca-78631ab7c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"欠損値を補完したデータの予測値\"\"\"\n",
    "#予測値と統計値を各科目のモデルごとにエクセルシートに出力\n",
    "\n",
    "\n",
    "kamoku=[\"国語\",\"英語\",\"数学\"]#,\"数学\"は文字数がないため、dropnaで除外してしまっている。前処理で変更する必要あり。\n",
    "model=[\"1年\",\"2年\",\"3年\",\"学年混合\"]\n",
    "\n",
    "mss= 2#min sample split\n",
    "msl= 1#min sample leaf\n",
    "md=None #max depth\n",
    "test_num=\"June3_解答言語\"\n",
    "\n",
    "name_file = \"DTs_{}_mss{}_msl{}_md{}.xlsx\".format(str(test_num),str(mss),str(msl),str(md)) #ファイル名\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/output\"#フォルダパス\n",
    "# path_file = r'{p}/{n}'.format(p = path_folder, n = name_file)#ファイルパスとファイル名\n",
    "\n",
    "# 入力データ\n",
    "df = df_ml_filled\n",
    "df1=[[[],[],[],[]],[[],[],[],[]],[[],[],[],[]]]\n",
    "df2=[[[],[],[],[]],[[],[],[],[]],[[],[],[],[]]]\n",
    "for i,n in enumerate(kamoku):\n",
    "    for j,m in enumerate(model):\n",
    "        df1[i][j] = test_model2(make_mldata2(df,kamoku[i])[j],mss,msl,md)\n",
    "        df2[i][j] = pandas_tool.summary(df1[i][j].loc[:,['最終ペース','企画ペース','AI想定ペース','誤差','AI乖離度','元の乖離度']])[0]\n",
    "        \n",
    "with pd.ExcelWriter('{}/{}'.format(path_folder,name_file)) as writer:\n",
    "    for i,n in enumerate(kamoku):\n",
    "        for j,m in enumerate(model):\n",
    "            s_names=['{}_{}_values'.format(kamoku[i],model[j]),'{}_{}_stats'.format(kamoku[i],model[j])]\n",
    "            df1[i][j].to_excel(writer, sheet_name='{}'.format(s_names[0]))\n",
    "            df2[i][j].to_excel(writer, sheet_name='{}'.format(s_names[1]))\n",
    "            \n",
    "            \n",
    "# =IFS(M2>=600,6,M2>=500,5,M2>=400,4,M2>=300,3,M2>=200,2,M2>=100,1,TRUE,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e6580048-4153-4ee6-9484-b3d37c4baf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/Users/s.ogura/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"欠損値を補完したデータの予測値\"\"\"\n",
    "#予測値と統計値を各科目のモデルごとにエクセルシートに出力\n",
    "\n",
    "\n",
    "kamoku=[\"国語\",\"英語\",\"数学\"]#,\"数学\"は文字数がないため、dropnaで除外してしまっている。前処理で変更する必要あり。\n",
    "model=[\"1年\",\"2年\",\"3年\",\"学年混合\"]\n",
    "\n",
    "mss= 2#min sample split\n",
    "msl= 1#min sample leaf\n",
    "md=None #max depth\n",
    "test_num=\"June3_全科目\"\n",
    "\n",
    "name_file = \"DTs_{}_mss{}_msl{}_md{}.xlsx\".format(str(test_num),str(mss),str(msl),str(md)) #ファイル名\n",
    "path_folder = r\"/Users/s.ogura/Documents/CRLEA/data/output\"#フォルダパス\n",
    "# path_file = r'{p}/{n}'.format(p = path_folder, n = name_file)#ファイルパスとファイル名\n",
    "\n",
    "# 入力データ\n",
    "df = df_ml_filled\n",
    "df1=[[[],[],[],[]],[[],[],[],[]],[[],[],[],[]]]\n",
    "df2=[[[],[],[],[]],[[],[],[],[]],[[],[],[],[]]]\n",
    "for i,n in enumerate(kamoku):\n",
    "    for j,m in enumerate(model):\n",
    "        df1[i][j] = test_model2(make_mldata_all(df,kamoku[i])[j],mss,msl,md)\n",
    "        df2[i][j] = pandas_tool.summary(df1[i][j].loc[:,['最終ペース','企画ペース','AI想定ペース','誤差','AI乖離度','元の乖離度']])[0]\n",
    "        \n",
    "with pd.ExcelWriter('{}/{}'.format(path_folder,name_file)) as writer:\n",
    "    for i,n in enumerate(kamoku):\n",
    "        for j,m in enumerate(model):\n",
    "            s_names=['{}_{}_values'.format(kamoku[i],model[j]),'{}_{}_stats'.format(kamoku[i],model[j])]\n",
    "            df1[i][j].to_excel(writer, sheet_name='{}'.format(s_names[0]))\n",
    "            df2[i][j].to_excel(writer, sheet_name='{}'.format(s_names[1]))\n",
    "            \n",
    "            \n",
    "# =IFS(M2>=600,6,M2>=500,5,M2>=400,4,M2>=300,3,M2>=200,2,M2>=100,1,TRUE,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
